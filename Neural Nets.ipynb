{"cells":[{"cell_type":"markdown","id":"3e5be36e","metadata":{"id":"3e5be36e"},"source":["# Neural Nets\n","\n","http://neuralnetworksanddeeplearning.com/"]},{"cell_type":"markdown","id":"f20471fc","metadata":{"id":"f20471fc"},"source":["## Neurons\n","\n","<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Components_of_neuron.jpg/640px-Components_of_neuron.jpg' alt='components of neuron' />"]},{"cell_type":"markdown","id":"9598a306","metadata":{"id":"9598a306"},"source":["By Jennifer Walinga - https://opentextbc.ca/introductiontopsychology/chapter/3-1-the-neuron-is-the-building-block-of-the-nervous-system/, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=97847412"]},{"cell_type":"markdown","id":"39a11018","metadata":{"id":"39a11018"},"source":["## Artificial Neuron (Perceptron)\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/Artificial_neural_network.png' alt='artificial neuron' />"]},{"cell_type":"markdown","id":"a7b8270f","metadata":{"id":"a7b8270f"},"source":["Artificial Neuron - By Geetika saini - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=57111539\n","artificial neuron consisting of dendrites,axon and threshold function"]},{"cell_type":"markdown","id":"0b7ad989","metadata":{"id":"0b7ad989"},"source":["* The diagram above is a single observation (one row)\n","* Each input represents a feature (explanatory variable)\n","* Weights are how neural nets learn (parameters, coefficients)\n","* The synapse is represented by the lines between the weights and the transfer function\n","* The output ($o_j$) can be continuous, binary, or categorical\n","* Probabilities are often attached to the categorical outputs\n","* Activation functions / formulas start are defined by $\\phi(x)$ or $\\phi(\\sum w_i x_i)$\n","* The type of activation function is based on the DV, for example if the DV is binary, we can use threshold or sigmoid activation functions\n","* Threshold: $y = \\phi(\\sum w_i x_i)$\n","* **OR** y = 1 if $\\sum w_i x_i$ ≥ θ, otherwise 0\n","* Sigmoid: $p(y=1) = \\phi(\\sum w_i x_i)$\n","* **OR** $p(y = 1) = σ(∑ w_i x_i)$, where $σ(x) = 1 / (1 + e^{-x})$"]},{"cell_type":"markdown","id":"3c6bfaa8","metadata":{"id":"3c6bfaa8"},"source":["## Transfer Function\n","\n","The transfer function translates the input signals to output signals. Four types of transfer functions are commonly used, Unit step (threshold), sigmoid, piecewise linear, and Gaussian. The output is set at one of two levels, depending on whether the total input is greater than or less than some threshold value.\n","\n","https://www.saedsayad.com/artificial_neural_network.htm"]},{"cell_type":"markdown","id":"b6e9c4d9","metadata":{"id":"b6e9c4d9"},"source":["## Activation Functions\n","\n","An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations.\n","\n","https://www.v7labs.com/blog/neural-networks-activation-functions\n","\n","### The Sigmoid: $S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = 1 - S(-x)$\n","\n","Learn more:\n","\n","* https://en.wikipedia.org/wiki/Sigmoid_function\n","* https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/\n","* https://en.wikipedia.org/wiki/Activation_function"]},{"cell_type":"markdown","id":"fd10f375","metadata":{"id":"fd10f375"},"source":["### Rectified Linear Unit (ReLU)\n","\n","* The Rectified Linear Unit is the most commonly used activation function in deep learning models. The function returns 0 if it receives any negative input, but for any positive value x it returns that value back.\n","* $f(x) = max(0, x)$\n","\n","and\n","\n","### Gaussian Error Linear Unit (GELU)\n","* Uses the standard Gaussian distribution, making it a smoother version of ReLU\n","* A common expression is: GELU(x) = $xΦ(x)$, where $Φ$ is the standard normal CDF\n"]},{"cell_type":"markdown","id":"23866732","metadata":{"id":"23866732"},"source":["<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/ReLU_and_GELU.svg' alt='ReLU' />"]},{"cell_type":"markdown","id":"f30a761c","metadata":{"id":"f30a761c"},"source":["## Some Neural Net Categories\n","\n","* Artificial Neural Nets: Feature selection / extraction\n","* Convolutional Neural Nets: Feature learning\n","* Recurrent Neural Nets: Feature propagation (memory)\n","* Propagation, in science, is the breeding of specimens of a plant or animal by natural processes from the parent stock"]},{"cell_type":"markdown","id":"d1a5fc31","metadata":{"id":"d1a5fc31"},"source":["## Artificial Neural Network (ANN)\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/Colored_neural_network.svg' alt='simple neural net' />\n","\n","ANN - By Glosser.ca - Own work, Derivative of File:Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461\n","An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n","ReLU - By Ringdongdang - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=95947821\n","Plot of the ReLU rectifier (blue) and GELU (green) functions near x = 0"]},{"cell_type":"markdown","id":"c178b356","metadata":{"id":"c178b356"},"source":["### A Simple Example Process\n","\n","* Initialize weights\n","* Observation features are passed through a hidden layer (forward propagation)\n","* Weights are adjusted, e.g., the third blue activation function might fire up, activate, when it receives inputs from the three red input features (shows importance)\n","* The two green outputs can be thought of as $\\hat{y}$, the predicted value, and is evaluated with y the actual value\n","* Cost function is calculated e.g., $\\frac{1}{2}(\\hat{y} - y)^2$ (cost function and gradient descent)\n","* Weights get adjusted (back propagation)\n","* Repeat (known as epochs) until some criteria is met such as cost function is minimized\n","* One epoch is when the whole training set is passed through the ANN\n","\n","### 3 Inputs Without the Hidden Layer\n","\n","* $y = w_1 x_1 + w_2 x_2 + w_3 x_3$\n","\n","### With a Hidden Layer\n","\n","* Inputs may or may not go to each node in the hidden layer\n","* Some inputs may not be relevant to some nodes\n","* Hidden layers are where features are discovered and selected (feature selection)\n","* $y = \\phi(\\sum w_i x_i)$\n","\n","### Batch Gradient Descent vs Stochastic Gradient Descent\n","\n","* Batch updates weights when all observations have been passed through the net\n","* SGD updates weights per observation"]},{"cell_type":"markdown","id":"486c24d1","metadata":{"id":"486c24d1"},"source":["### Output Probabilities\n","\n","* Output layer using softmax to provide probabilities for each label such as Setosa, Versicolor, Virginica\n","* Output shows probabilities for each label in the class (.3, .2, .5)"]},{"cell_type":"markdown","id":"67403eac","metadata":{"id":"67403eac"},"source":["## Another Neural Net\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/neural%20network.jpg' alt='neural network' />\n","\n","Neural Network - \"Neural Network : basic scheme with legends\" by fdecomite is licensed under CC BY 2.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/2.0/?ref=openverse."]},{"cell_type":"markdown","id":"d47c4ff4","metadata":{"id":"d47c4ff4"},"source":["## Deep Neural Net\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/deep%20neural%20net.jpg' alt='deep neural net' />\n","\n","Deep Neural Net - https://www.vectorstock.com/royalty-free-vector/neural-net-neuron-network-vector-10723960"]},{"cell_type":"markdown","id":"0e5dd214","metadata":{"id":"0e5dd214"},"source":["## MLP (Multi-Layer Perceptron)\n","\n","Only goes one direction, feed forward"]},{"cell_type":"markdown","id":"c6b219a6","metadata":{"id":"c6b219a6"},"source":["## Backpropagation\n","\n","* Neuron output is compared to actual value using $C = \\frac{1}{2}(\\hat{y} - y)^2$\n","* Weights are then updated according to how much they are responsible for the error (such as MSE)\n","* The learning rate decides by how much we update the weights\n","* Adjust all weights simultaneously\n","\n","As a machine-learning algorithm, backpropagation performs a backward pass to adjust the model's parameters, aiming to minimize the mean squared error (MSE). In a single-layered network, backpropagation uses the following steps:\n","\n","* Traverse through the network from the input to the output by computing the hidden layers' output and the output layer (The Feedforward Step)\n","* In the output layer, calculate the derivative of the cost function with respect to the input and the hidden layers\n","* Repeatedly update the weights until they converge or the model has undergone enough iterations\n","\n","https://en.wikipedia.org/wiki/Backpropagation"]},{"cell_type":"markdown","id":"fc049e1d","metadata":{"id":"fc049e1d"},"source":["## CNN (Convolutional Neural Network)\n","\n","https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n","\n","Gradient-Based Learning Applied to Document Recognition by Yann LeCun et al. (1998)\n","\n","http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\n","\n","### CNN and Document Classification\n","\n","Convolutional neural networks are effective at document classification, namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences.\n","\n","https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n","\n","Networks with convolutional and pooling layers are useful for classification tasks in which we expect to find strong local clues regarding class membership, but these clues can appear in different places in the input. […] We would like to learn that certain sequences of words are good indicators of the topic, and do not necessarily care where they appear in the document. Convolutional and pooling layers allow the model to learn to find such local indicators, regardless of their position.\n","\n","https://arxiv.org/abs/1510.00726\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/Convolutional_Neural_Network_with_Color_Image_Filter.gif' alt='convolutional neural network' />\n","\n","By Cecbur - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=76640332"]},{"cell_type":"code","execution_count":null,"id":"435b6ffa","metadata":{"id":"435b6ffa","outputId":"5145fb26-2799-470b-bc84-cf809f8d043a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"," [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"," [1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]]\n","[[1 0 1]\n"," [0 1 0]\n"," [1 0 1]]\n"]}],"source":["import numpy as np\n","n = 15\n","a = np.zeros(n*n, dtype=int)\n","a[::2]=1\n","a = a.reshape(n, n)\n","print(a)\n","n = 3\n","a = np.zeros(n*n, dtype=int)\n","a[::2]=1\n","a = a.reshape(n, n)\n","print(a)"]},{"cell_type":"markdown","id":"9d7bb4f9","metadata":{"id":"9d7bb4f9"},"source":["## CNN Steps\n","\n","### Convolution\n","\n","* Imagine an input image with a series of 0s and 1s\n","* We take a nxm feature detector (filter) which is also a series of 0s and 1s and let it stride across our image to see if there are any similar nxm sections\n","* A feature map (convolved map) records the results of an element wise multiplication for matching patterns\n","* This reduces the size of our *image*, lossy, but the feature map preserves the discovered features\n","* We create many feature maps to create a convolution layer\n","* A convolutional layer is a layer of feature maps\n","\n","### ReLU Layer\n","\n","* Now we apply a rectifier such as $\\phi(x) = max(x, 0)$\n","\n","### Max Pooling\n","\n","* Example, find the max value from a stride and record it to a pooled feature map\n","* This reduces the size again and preserve the features\n","* Disregards noise\n","* Prevents overfitting\n","\n","### Flattening\n","\n","* This step takes a matrix and flattens it to a 1xn stack\n","\n","### Full Connection\n","\n","* From here, the flattened stack gets fed into an ANN"]},{"cell_type":"markdown","id":"aeb87268","metadata":{"id":"aeb87268"},"source":["## RNN (Recurrent Neural Network)\n","\n","Called recurrent because they perform the same task for every element of a sequence with the output being dependent on the previous computations (memory)\n","\n","* Time series\n","* Sequential data\n","* Recognizes sequential patterns and tries to predict the next likely event\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/Recurrent_neural_network_unfold.svg' alt='recurrent neural network' />\n","\n","By fdeloche - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157\n","\n","* The bottom input is the state\n","* The middle is the hidden state\n","* The top is the output state\n","* U, V, W are the weights matrices of the hidden layer, output layer, and hidden state respectively and are shared across time\n","* Compressed version on the left, the unfolded version on the right (represents time, or words in a sentence)\n","* RNNs suffer from exploding or vanishing gradients"]},{"cell_type":"markdown","id":"b551d88f","metadata":{"id":"b551d88f"},"source":["## Long Short Term Memory\n","\n","Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video).\n","\n","https://en.wikipedia.org/wiki/Long_short-term_memory\n","\n","* Cell state - information flows from left to right unchanged\n","* Information can be added or removed from the cell state and are regulated by gates\n","* LSTMs remembers or forgets things selectively which is different from the RNN\n","\n","<img src='https://raw.githubusercontent.com/gitmystuff/Linkables/main/LSTM_Cell.svg' alt='LSTM Cell' />\n","\n","By Guillaume Chevalier - File:The_LSTM_Cell.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109362147"]},{"cell_type":"markdown","id":"211601fe","metadata":{"id":"211601fe"},"source":["\n","## Notes\n","\n","Relu returns 0 or a positive number\n","\n","Softmax activation for classification\n","\n","Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.\n","\n","You might recall that information quantifies the number of bits required to encode and transmit an event. Lower probability events have more information, higher probability events have less information.\n","\n","In information theory, we like to describe the “surprise” of an event. An event is more surprising the less likely it is, meaning it contains more information.\n","\n","https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n","\n","Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n","\n","The algorithm is called Adam. It is not an acronym and is not written as “ADAM”.\n","\n","https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}