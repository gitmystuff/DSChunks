{"cells":[{"cell_type":"markdown","id":"6dd35102","metadata":{"id":"6dd35102"},"source":["\n","\n","## Boosting\n","\n","Boosting is an **ensemble meta-algorithm** that combines multiple **weak learners** sequentially to create a single, highly accurate **strong learner**.\n","\n","* **Weak Learner:** A model (usually a simple decision tree) whose performance is only slightly better than random guessing (e.g., an accuracy of 51% in a binary classification).\n","* **Strong Learner:** The final aggregated model that achieves arbitrarily high accuracy and is strongly correlated with the true classification.\n","\n","***\n","\n","## The Sequential Boosting Process\n","\n","Boosting is fundamentally a sequential process where each new model is built to compensate for the weaknesses of the combined previous models.\n","\n","| Step | Action | Statistical Goal |\n","| :--- | :--- | :--- |\n","| **1. Initialization** | A weak base learner is trained on the initial dataset. | Establish a baseline prediction. |\n","| **2. Prioritize Errors (Data Weights)** | The model identifies instances it **misclassified**. These misclassified data points are assigned **higher weights**, making them more \"important\" for the next learner. | Focus the subsequent learner on the hardest-to-classify samples. |\n","| **3. Sequential Training** | A new weak learner is trained on the now-reweighted dataset, forced to focus on the previously misclassified points. | Reduce the systematic error (Bias) by specializing the new model. |\n","| **4. Reward Accuracy (Learner Weights)** | Each new weak learner is assigned a **vote weight** based on its *individual accuracy* on the training data. **More accurate** learners are given **larger weights**. | Ensure the most reliable models contribute the most to the final prediction. |\n","| **5. Aggregation** | The process continues until a desired performance level or a set number of learners is reached. The final strong learner aggregates the predictions from all weak learners using their calculated vote weights. | Produce a final robust, low-bias prediction. |\n","\n","***\n","\n","## Key Clarifications: Weights and Ensembles\n","\n","### 1. The Role of Weights\n","\n","| Item | What is Weighted? | Goal |\n","| :--- | :--- | :--- |\n","| **Data Weights** | The individual data points in the training set. | **Direct the Learning:** Emphasize misclassified samples for the *next* sequential model to correct. |\n","| **Learner Weights** | The final prediction (or vote) of each weak model. | **Combine the Votes:** Give more influence to the *more accurate* models in the final ensemble prediction. |\n","\n","### 2. Boosting vs. Stacking/Blending\n","\n","The final point about \"Stacking and blending\" is **incorrect** as a description of Boosting. They are distinct ensemble methods:\n","\n","* **Boosting:** **Sequential** ensemble method where models are trained one after another, correcting previous errors. (e.g., AdaBoost, Gradient Boosting, XGBoost).\n","* **Bagging:** **Parallel** ensemble method where models are trained independently on different bootstrap samples of the data. (e.g., Random Forest).\n","* **Stacking/Blending:** **Layered** ensemble method where multiple diverse models are trained first, and then a final meta-model (or blender) is trained to combine their predictions."]},{"cell_type":"markdown","id":"2da7e13e","metadata":{"id":"2da7e13e"},"source":["##Adaptive Boosting\n","\n","**AdaBoost** (Adaptive Boosting) is a foundational and highly effective boosting algorithm. Its primary goal is to **reduce bias** by training a sequence of simple, weak models that focus on correcting the errors made by their predecessors.\n","\n","***\n","\n","## The Iterative Training Process\n","\n","AdaBoost relies on **re-weighting** both the data and the models at each step of the process:\n","\n","1.  **Initialization:** All observations (data points) in the training set are assigned an **equal weight**.\n","2.  **Build Weak Learner:** A simple predictive model, often a decision tree with maximum depth of one (a **decision stump**), is built on the currently weighted dataset.\n","3.  **Re-Weight Data (Prioritize Errors):** The algorithm identifies all **misclassified observations**. The weights of these misclassified points are **increased**, making them more influential in the training of the next model. Conversely, the weights of correctly classified points are decreased.\n","4.  **Calculate Learner Weight (Reward Accuracy):** The current weak model is assigned a **vote weight** based on its overall accuracy. Models that perform better are given a proportionally **larger weight** for the final ensemble prediction.\n","5.  **Iteration:** Steps 2 through 4 are **repeated** until a predefined maximum number of estimators is reached, or until the overall error rate stabilizes or falls below a certain threshold.\n","6.  **Aggregation:** The final prediction is a **weighted majority vote** of all the weak learners, using the weights calculated in Step 4.\n","\n","***\n","\n","## Key Characteristics\n","\n","* **Weak Learners:** AdaBoost uses simple learners, typically a **decision stump** (a decision tree with a $\\text{max\\_depth}$ of $1$). These models are **high-bias** and **low-variance**, which helps prevent the overall powerful ensemble from becoming too complex and overfitting.\n","* **Focus on Bias:** The aggressive re-weighting of data effectively forces the ensemble to specialize in the samples that are difficult to classify, rapidly driving down the systematic error, or **bias**, of the model."]},{"cell_type":"code","execution_count":null,"id":"05ee7fa9","metadata":{"id":"05ee7fa9","outputId":"0bea984d-23a5-4824-912c-37a07039980d"},"outputs":[{"data":{"text/plain":["0.983"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.datasets import make_classification\n","\n","X, y = make_classification(n_samples=1000, n_features=4,\n","                           n_informative=2, n_redundant=0,\n","                           random_state=0, shuffle=False)\n","\n","clf = AdaBoostClassifier(n_estimators=100, random_state=0).fit(X, y)\n","clf.predict([X[0]])\n","clf.score(X, y)"]},{"cell_type":"markdown","id":"06d69360","metadata":{"id":"06d69360"},"source":["\n","\n","## Gradient Boosting\n","\n","**Gradient Boosting** is a sequential ensemble technique that builds models (typically decision trees) not on the target variable ($y$) directly, but on the **residual errors** (or \"pseudo-residuals\") of the previous combined predictions. Its primary goal is to minimize a loss function (error) by moving down the gradient (like in optimization).\n","\n","### The Iterative Process\n","\n","1.  **Initial Model Deployment:** An initial, simple model ($F_0(x)$), often a constant value (like the mean of the target variable), is used to make predictions across the entire dataset.\n","2.  **Calculate Residuals (Errors):** The algorithm calculates the **residual error** (or loss gradient) between the true values ($y$) and the current ensemble's predictions ($F_{m-1}(x)$). This residual is the direction of the steepest descent for the loss function.\n","3.  **Train a New Weak Learner:** A new weak learner ($h_m(x)$), usually a shallow decision tree, is trained. However, it's trained to predict the **residuals ($r_i$)** from the previous step, *not* the original target variable ($y$).\n","4.  **Merge Predictions (Update Ensemble):** The new weak learner's prediction ($h_m(x)$) is scaled by a learning rate and then **added** to the previous ensemble's prediction ($F_{m-1}(x)$) to create the new, improved ensemble prediction ($F_m(x)$).\n","   \n","5.  **Compute New Residuals:** New residuals are computed based on the improved predictions from the updated ensemble $F_m(x)$.\n","6.  **Repeat:** The process is repeated iteratively until the error function stops improving, or a pre-defined limit of estimators is reached.\n","\n","### Key Distinction from AdaBoost\n","\n","* **AdaBoost:** Adjusts **data weights** to focus the next learner.\n","* **Gradient Boosting:** Adjusts the **target variable** for the next learner (it sets the target to the residual error), forcing the new model to explicitly learn what the previous ensemble missed."]},{"cell_type":"code","execution_count":null,"id":"91f27840","metadata":{"id":"91f27840","outputId":"e9ad864c-a4dd-4c4a-a66d-f0ede603f0d4"},"outputs":[{"data":{"text/plain":["0.936"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.datasets import make_classification\n","\n","X, y = make_classification(n_samples=1000, n_features=4,\n","                           n_informative=2, n_redundant=0,\n","                           random_state=42, shuffle=False)\n","\n","clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n","                                 max_depth=1, random_state=42).fit(X, y)\n","clf.predict([X[0]])\n","clf.score(X, y)"]},{"cell_type":"markdown","id":"272da592","metadata":{"id":"272da592"},"source":["## XGBoost (Extreme Gradient Boosting)\n","\n","* https://www.youtube.com/watch?v=OtD8wVaFm6E\n","* Fast\n","* Uses regularization techniques\n","* Reduces overfitting\n","* Improves overall performance\n","* Uses parallel processing\n","* Customizable optimization objectives and evaluation criteria\n","* Builtin routine to handle missing values"]},{"cell_type":"code","execution_count":null,"id":"d059a6a4","metadata":{"id":"d059a6a4"},"outputs":[],"source":["# pip install xgboost"]},{"cell_type":"code","execution_count":null,"id":"f75af835","metadata":{"id":"f75af835","outputId":"27bfb1b2-5c20-4b6e-ea65-1e7e0fa1f5f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.956140350877193\n"]}],"source":["# https://xgboost.readthedocs.io/en/stable/parameter.html\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_breast_cancer\n","\n","cancer = load_breast_cancer()\n","df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n","df['class'] = cancer.target\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.2, random_state=42)\n","\n","model = XGBClassifier(booster='gbtree', eta=0.3, max_depth=6, seed=42).fit(X_train, y_train)\n","predictions = model.predict(X_test)\n","print(accuracy_score(predictions, y_test))"]},{"cell_type":"markdown","id":"86068863","metadata":{"id":"86068863"},"source":["## Light GBM\n","\n","* https://lightgbm.readthedocs.io/en/stable/\n","* Supports parallel, distributed, and GPU learning"]},{"cell_type":"markdown","id":"af633ceb","metadata":{"id":"af633ceb"},"source":["## CATBoost\n","\n","* https://catboost.ai/\n","* https://catboost.ai/en/docs/concepts/python-quickstart"]},{"cell_type":"markdown","id":"02679c60","metadata":{"id":"02679c60"},"source":["## When to Use Which\n","\n","* https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}