{"cells":[{"cell_type":"markdown","id":"bc51a40a","metadata":{"id":"bc51a40a"},"source":["# Regularization\n","\n","* Penalizes complex models to prevent overfitting\n","\n","https://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\n","\n","In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of solving ill-posed problems or to prevent overfitting. Regularization can be applied to objective functions in ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.\n","\n","https://en.wikipedia.org/wiki/Regularization_(mathematics)\n","\n","## Cost or Loss Function\n","\n","In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.\n","\n","We often use MSE as our cost function in linear regression $mse = \\frac{\\sum(y-\\hat{y})^2}{n}$.\n","\n","https://en.wikipedia.org/wiki/Loss_function"]},{"cell_type":"code","execution_count":null,"id":"e827da3a","metadata":{"id":"e827da3a"},"outputs":[],"source":["# get data and train test split\n","import numpy as np\n","import pandas as pd\n","\n","auto = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/Auto.csv', usecols=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year'])\n","auto = auto[(auto != '?').all(axis=1)]\n","auto['horsepower'] = auto['horsepower'].astype(np.int64)\n","\n","# train test split\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    auto.drop('mpg', axis=1),\n","    auto['mpg'],\n","    test_size=0.25,\n","    random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"f903961f","metadata":{"id":"f903961f","outputId":"18d1992b-9768-4f5b-c4df-36e0ebc84415"},"outputs":[{"data":{"text/plain":["cylinders      -0.160143\n","displacement    0.000373\n","horsepower     -0.001899\n","weight         -0.006457\n","acceleration    0.057588\n","year            0.762270\n","dtype: float64"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# remind us of the OLS coefficients\n","import statsmodels.api as sm\n","\n","# add the constant\n","# X_train = sm.add_constant(X_train)\n","X_train.insert(0, 'const', 1)\n","model = sm.OLS(y_train, X_train[['const', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year']]).fit()\n","model.params[1:]"]},{"cell_type":"code","execution_count":null,"id":"78759f41","metadata":{"id":"78759f41","outputId":"e778d474-c5e0-4b9d-b18c-102e760845c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["        Feature     Coeff\n","1     cylinders -0.000000\n","2  displacement -0.000000\n","3    horsepower -0.000000\n","4        weight -1.670802\n","5  acceleration  0.000000\n","6          year  0.000000\n"]}],"source":["# lasso example\n","from sklearn.linear_model import Lasso\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X_train)\n","y = y_train\n","names = auto.columns\n","\n","lasso = Lasso(alpha=5)\n","lasso.fit(X, y)\n","\n","d = {'Feature': names, 'Coeff': lasso.coef_}\n","lasso_df = pd.DataFrame(d)\n","print(lasso_df[1:])"]},{"cell_type":"markdown","id":"b8a620f6","metadata":{"id":"b8a620f6"},"source":["## Lasso / l1 Regularization\n","\n","* $\\alpha = \\sum|w_i|$\n","* Forces weak features to have zero coefficients\n","* Performs feature selection\n","* Models can be unstable (coefficients fluctuate significantly on data changes with correlated features)"]},{"cell_type":"code","execution_count":null,"id":"f91e672c","metadata":{"scrolled":false,"id":"f91e672c","outputId":"ed00162b-fca4-4208-bf9a-3166950a90a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["        Feature     Coeff\n","1     cylinders -0.141583\n","2  displacement  0.000050\n","3    horsepower -0.002074\n","4        weight -0.006451\n","5  acceleration  0.056597\n","6          year  0.759790\n"]}],"source":["# ridge example\n","from sklearn.linear_model import Ridge\n","\n","X=X_train\n","y=y_train\n","\n","ridge = Ridge(alpha=10)\n","ridge.fit(X,y)\n","\n","d = {'Feature': names, 'Coeff': ridge.coef_}\n","ridge_df = pd.DataFrame(d)\n","print(ridge_df[1:])"]},{"cell_type":"markdown","id":"b6de2dc1","metadata":{"id":"b6de2dc1"},"source":["## Ridge / l2 Regularization\n","\n","* $\\alpha = \\sum w_i^2$\n","* Spreads out coefficients more equally\n","* Exposes correlated features (have similar coefficients)\n","* Models are more stable (coefficients don't fluctuate as much on data changes with correlated features)"]},{"cell_type":"markdown","id":"25c8af97","metadata":{"id":"25c8af97"},"source":["## ElasticNet"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}