{"cells":[{"cell_type":"markdown","id":"a132e02a","metadata":{"id":"a132e02a"},"source":["# Ensemble Learning"]},{"cell_type":"markdown","id":"827d3983","metadata":{"id":"827d3983"},"source":["\n","\n","## Bagging (Bootstrap Aggregating)\n","\n","Bagging is an ensemble meta-algorithm designed primarily to **reduce variance** and improve the **stability and accuracy** of machine learning models, helping to avoid **overfitting**.\n","\n","### The Mechanism: Parallel and Independent\n","\n","Bagging uses a technique called **bootstrapping** (sampling with replacement) to create multiple, diverse datasets.\n","\n","1.  **Bootstrap Sampling:** The process involves repeatedly taking a **random sample with replacement** from the original training dataset.\n","2.  **Independent Training:** Models (often decision trees) are **trained independently and in parallel** on each of these newly generated bootstrap samples.\n","3.  **Aggregation (Averaging):** The final prediction is obtained by **averaging** the predictions of all individual models (for regression) or by taking a **majority vote** (for classification).\n","4.  **Extension:** **Random Forest** is a highly successful extension of Bagging that adds an extra layer of randomness by considering only a subset of features at each split.\n","\n","### Primary Goal: Reduce Variance\n","\n","Because the individual models are trained on slightly different subsets of data, their errors tend to cancel each other out during aggregation, resulting in a more stable and generalized model with **lower variance**.\n","|https://www.ibm.com/cloud/learn/bagging\n","\n","### Boosting\n","\n","In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n","\n","https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n","\n","* Great for reducing bias and variance\n","* Train a weak learner such as a classifier that performs slightly better than random guessing\n","* Find misclassifications and train another weak learner that focuses on the misclassifications\n","* Use weights to reward, or emphasize, the weak learner (large weights) and penalize the strong learner (small weights)\n","* Data is also weighted, misclassified data are weighted as more important than correctly classified data\n","* Each weak learner learns from the previous learner\n","* Continue process until desired output is reached and then aggregate all the learners\n","\n","<img src='https://miro.medium.com/max/1200/1*zTgGBTQIMlASWm5QuS2UpA.jpeg' alt='bagging vs boosting' />\n","\n","Sources:\n","\n","* Figure 1. Bagging and Boosting | Spreadsheet, Robot and Idea icons by Freepik on Flaticon\n","* https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}