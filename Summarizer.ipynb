{"cells":[{"cell_type":"markdown","metadata":{"id":"VIOWH2uF6Lfi"},"source":["# Summarizer\n","\n","### Wikipedia API\n","\n","If you intend to do any scraping projects or automated requests, consider alternatives such as Pywikipediabot or MediaWiki API, which has other superior features.\n","\n","* wikipedia.search('keywords', results=2)\n","* wikipedia.suggest('keyword')\n","* wikipedia.summary('keywords', sentences=2)\n","* wikipedia.page('keywords')\n","* wikipedia.page('keywords').content\n","* wikipedia.page('keywords').references\n","* wikipedia.page('keywords').title\n","* wikipedia.page('keywords').url\n","* wikipedia.page('keywords').categories\n","* wikipedia.page('keywords').content\n","* wikipedia.page('keywords').links\n","* wikipedia.geosearch(33.2075, 97.1526)\n","* wikipedia.set_lang('hi')\n","* wikipedia.languages()\n","* wikipedia.page('keywords').images[0]\n","* wikipedia.page('keywords').html()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SktsLUI-6Lfj"},"outputs":[],"source":["# pip install wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dR3j7pe6Lfk"},"outputs":[],"source":["# https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n","import pandas as pd\n","import random\n","import wikipedia\n","\n","# rtitles = wikipedia.random(5)\n","\n","# get 5 Wikipedia page titles based on keywords or manually enter in keywords list\n","titles = []\n","keywords = ['Titanic', 'JP Morgan', 'immigration', 'suffrage', 'racist']\n","for key in keywords:\n","    title = wikipedia.search(key, results=1)\n","    titles.append(title[0])\n","\n","print(titles)\n","data = []\n","\n","for title in titles:\n","    # disambiguous error fix\n","    try:\n","        data.append([title, wikipedia.page(title, auto_suggest=False).content, wikipedia.summary(title, auto_suggest=False, sentences=5)])\n","    except wikipedia.exceptions.DisambiguationError as e:\n","        s = random.choice(e.options)\n","        data.append([title, wikipedia.page(s).content,  wikipedia.summary(title, auto_suggest=False, sentences=5)])\n","\n","df = pd.DataFrame(data, columns=['title', 'content', 'summary'])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"pbjGUW-Y6Lfk"},"source":["## Summarization Using spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hng49naq6Lfk"},"outputs":[],"source":["# https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from string import punctuation\n","from collections import Counter\n","from heapq import nlargest\n","\n","nlp = spacy.load('en_core_web_md')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ye29DRkN6Lfk"},"outputs":[],"source":["# doc = nlp(df.loc[0]['content'])\n","summary_text = ' '.join([txt for txt in df.summary])\n","# print(summary_text)\n","doc = nlp(summary_text)\n","len(list(doc.sents))\n","keyword = []\n","stopwords = list(STOP_WORDS)\n","pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n","for token in doc:\n","    if(token.text in stopwords or token.text in punctuation):\n","        continue\n","    if(token.pos_ in pos_tag):\n","        keyword.append(token.text)\n","\n","# count most frequent words\n","freq_word = Counter(keyword)\n","print(freq_word.most_common(5))\n","\n","# normalize for better processing\n","max_freq = Counter(keyword).most_common(1)[0][1]\n","for word in freq_word.keys():\n","    freq_word[word] = (freq_word[word]/max_freq)\n","\n","print(freq_word.most_common(5))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WouOx4G36Lfk"},"outputs":[],"source":["# weights based on frequency\n","sent_strength={}\n","for sent in doc.sents:\n","    for word in sent:\n","        if word.text in freq_word.keys():\n","            if sent in sent_strength.keys():\n","                sent_strength[sent] += freq_word[word.text]\n","            else:\n","                sent_strength[sent] = freq_word[word.text]\n","\n","print(sent_strength)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2aS-pTJ6Lfk"},"outputs":[],"source":["summary = nlargest(10, sent_strength, key=sent_strength.get)\n","summary = ' '.join([w.text for w in summary])\n","summary"]},{"cell_type":"markdown","metadata":{"id":"BBZo_CHv6Lfl"},"source":["## Summarization Using Hugging Face\n","\n","* Limit 512 tokens (BART 1024)\n","* Models: https://huggingface.co/docs/transformers/index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdmRLKBl6Lfl"},"outputs":[],"source":["# pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkviAmE96Lfl"},"outputs":[],"source":["import pandas as pd\n","from transformers import pipeline\n","\n","summary_text = ' '.join([txt for txt in df.summary])\n","model = pipeline('summarization')\n","model(summary_text, min_length=300, max_length=400)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vuw8anIa6Lfl"},"outputs":[],"source":["# https://huggingface.co/facebook/bart-large-cnn\n","import pandas as pd\n","from transformers import pipeline\n","\n","summary_text = ' '.join([txt for txt in df.summary])\n","model = pipeline('summarization', model='facebook/bart-large-cnn')\n","model(summary_text, min_length=300, max_length=400)"]},{"cell_type":"markdown","metadata":{"id":"9KS28wnP6Lfl"},"source":["## Original Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9V_VmUg6Lfl"},"outputs":[],"source":["print('Word Count:', len(summary_text.split(' ')))\n","print(summary_text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}