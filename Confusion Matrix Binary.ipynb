{"cells":[{"cell_type":"markdown","id":"1148ca0d","metadata":{"id":"1148ca0d"},"source":["# Confusion Matrix"]},{"cell_type":"code","execution_count":null,"id":"e88e170c","metadata":{"id":"e88e170c","outputId":"1d393d5e-cbf0-4ea1-9b2a-8bb72c224c83"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature_1</th>\n","      <th>Feature_2</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.355723</td>\n","      <td>0.932216</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.448966</td>\n","      <td>0.628289</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.259349</td>\n","      <td>-0.792658</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.815272</td>\n","      <td>-0.205096</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.430197</td>\n","      <td>1.342170</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Feature_1  Feature_2  Class\n","0  -1.355723   0.932216      0\n","1  -1.448966   0.628289      0\n","2  -0.259349  -0.792658      0\n","3   0.815272  -0.205096      1\n","4   1.430197   1.342170      1"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Create dataset\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","\n","n = 1000 # number of observations\n","f = 2 # number of features\n","inform = 2 # number of meaningful features\n","features, binary_class = make_classification(n_samples=n, n_features=f,\n","                                             n_informative=inform, n_redundant=0,\n","                                             n_clusters_per_class=1, random_state=13)\n","\n","# Create a dataframe of the features and add the binary class (label, output)\n","df = pd.DataFrame(features)\n","df.columns = ['Feature_1', 'Feature_2']\n","df['Class'] = binary_class\n","df.head()"]},{"cell_type":"markdown","id":"a337a642","metadata":{"id":"a337a642"},"source":["Here is some tabular data and each observation is labeled either a 0 or 1. Our goal is to create a model and predict whether new data will be labelled a 0 or a 1."]},{"cell_type":"code","execution_count":null,"id":"6a61283f","metadata":{"id":"6a61283f"},"outputs":[],"source":["# X_train, X_test split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=0.20)\n","\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"code","execution_count":null,"id":"66005c9b","metadata":{"id":"66005c9b"},"outputs":[],"source":["# Create model\n","from sklearn.linear_model import LogisticRegression\n","\n","model = LogisticRegression(solver='liblinear')\n","model.fit(X_train,y_train)\n","predictions = model.predict(X_test)"]},{"cell_type":"markdown","id":"66b7e244","metadata":{"id":"66b7e244"},"source":["**The Solver (liblinear)**: Solvers are used to minimize the loss function which in turn provides our best coefficients. Please see the documentation to learn about the five different solvers.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"]},{"cell_type":"code","execution_count":null,"id":"0d6c34b5","metadata":{"id":"0d6c34b5","outputId":"7b7f37d8-2296-4f60-dd01-baf209cd3326"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix\n","[[79 14]\n"," [12 95]]\n","\n","Here is what the numbers represent from the sklLearn output\n","\n","                 predicted\n","                   |  0  |  1\n","           --------------------  \n","           class 0 | TN  |  FP      \n","  actual   --------------------\n","           class 1 | FN  |  TP\n","\n","\n","Here is what the numbers represent from the Wikipedia article\n","\n","                 actual\n","                   |  1  |  0\n","           --------------------  \n","           class 1 | TP  |  FP      \n","predicted  --------------------\n","           class 0 | FN  |  TN\n","\n"]}],"source":["# View confusion matrix\n","from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n","\n","tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n","print('Confusion Matrix')\n","print(confusion_matrix(y_test, predictions))\n","print()\n","print('Here is what the numbers represent from the sklLearn output')\n","print('''\n","                 predicted\n","                   |  0  |  1\n","           --------------------\n","           class 0 | TN  |  FP\n","  actual   --------------------\n","           class 1 | FN  |  TP\n","''')\n","print()\n","print('Here is what the numbers represent from the Wikipedia article')\n","print('''\n","                 actual\n","                   |  1  |  0\n","           --------------------\n","           class 1 | TP  |  FP\n","predicted  --------------------\n","           class 0 | FN  |  TN\n","''')"]},{"cell_type":"code","execution_count":null,"id":"f7f60915","metadata":{"id":"f7f60915","outputId":"a367b7c7-35ee-4d25-ba08-12ab688980f4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>All</th>\n","    </tr>\n","    <tr>\n","      <th>Actual</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>79</td>\n","      <td>14</td>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12</td>\n","      <td>95</td>\n","      <td>107</td>\n","    </tr>\n","    <tr>\n","      <th>All</th>\n","      <td>91</td>\n","      <td>109</td>\n","      <td>200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Predicted   0    1  All\n","Actual                 \n","0          79   14   93\n","1          12   95  107\n","All        91  109  200"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Compare with crosstab\n","pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'],margins=True)"]},{"cell_type":"markdown","id":"d91fc0d9","metadata":{"id":"d91fc0d9"},"source":["### Confusion Matrix\n","\n","\n","\n","A **Confusion Matrix** (or **error matrix**) is a specific type of contingency table used in machine learning to **visualize and summarize the performance** of a classification algorithm.\n","\n","It is always a square matrix, with the dimensions representing the **actual classes** and the **predicted classes**, both of which use the identical set of categories.\n","\n","### The Four Outcomes (The Cells)\n","\n","For a binary classification problem (e.g., classifying emails as spam or not-spam), the $2 \\times 2$ matrix provides four possible outcomes, which form the basis of all performance metrics:\n","\n","| | **Predicted Positive** | **Predicted Negative** |\n","| :--- | :--- | :--- |\n","| **Actual Positive** | **True Positive (TP):** Correctly predicted the positive class. | **False Negative (FN):** Incorrectly predicted the negative class (Type II Error). |\n","| **Actual Negative** | **False Positive (FP):** Incorrectly predicted the positive class (Type I Error). | **True Negative (TN):** Correctly predicted the negative class. |\n","\n","### Derived Metrics\n","\n","The power of the confusion matrix lies in its ability to isolate these four outcomes, allowing us to derive crucial performance metrics:\n","\n","1.  **Accuracy:** The proportion of total predictions that were correct.\n","    $$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n","2.  **Precision (Positive Predictive Value):** Of all instances predicted as positive, how many were actually correct?\n","    $$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n","3.  **Recall (Sensitivity):** Of all actual positive instances, how many were correctly identified?\n","    $$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n","\n","https://en.wikipedia.org/wiki/Confusion_matrix\n","\n","**Note**: Because of the of the output sklearn provides, we will structure our table different from what Wikipedia says.\n","\n","### Type I and Type II Errors\n","\n","https://towardsdatascience.com/statistics-for-data-scientists-f2456d26c5a5\n","\n","Type I Error means rejecting the True null hypothesis and accepting the alternate hypothesis. It is a false positive. A smoke detector detecting smoke when there is no smoke.\n","Type II Error means accepting the null hypothesis when an alternate hypothesis is true. It is a false negative. When fire alarm fails to detect fire.\n","\n","* **True Positive (TP)**: An outcome that is labeled 1 and in reality is a 1\n","\n","* **False Positive (FP)**: An outcome that is labeled 1 and in reality is a 0 (Type I Error)\n","\n","* **False Negative (FN)**: An outcome that is labeled 0 and in reality is a 1 (Type II Error)\n","\n","* **True Negative (TN)**: An outcome that is labeled 0 and in reality is a 0\n","\n","* **Accuracy**: (TP + TN) / (TP + FP + TN + FN)\n","\n","* **Precision**: TP / (TP + FP), a measure of quality\n","\n","* **Recall**: TP / (TP + FN), a measure of quantity"]},{"cell_type":"markdown","id":"de7fb3b2","metadata":{"id":"de7fb3b2"},"source":["### Underfitting and Overfitting\n","\n","In statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\". An overfitted model is a statistical model that contains more parameters than can be justified by the data.\n","\n","<img src='https://docs.aws.amazon.com/images/machine-learning/latest/dg/images/mlconcepts_image5.png' alt='underfitting overfitting' />\n","\n","https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n","\n","If you're overfitting, or you are getting great training scores and poor  test scores, you might be overfitting so try removing the lesser performing features. The model is just memorizing the training data.\n","\n","Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing.[2] Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\n","\n","If you're underfitting, or you are getting poor training scores and test scores, you might be underfitting so try adding more data or more features.\n","\n","https://en.wikipedia.org/wiki/Overfitting"]},{"cell_type":"markdown","id":"958ec329","metadata":{"id":"958ec329"},"source":["### Bias Variance Tradeoff\n","\n","In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters. The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n","\n","https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n","\n","Please become familiar with this image:\n","\n","<img src='https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg' alt='bias variance tradeoff' />\n","\n","Please read the article responsible for the image:\n","\n","https://www.kdnuggets.com/2016/08/bias-variance-tradeoff-overview.html\n","\n","#### A Formula\n","\n","$E[(\\hat{y} - y)^2] = E[(\\hat{y} - E[\\hat{y})^2] + (E[\\hat{y}] - y)^2$\n","<br />MSE = Variance + Bias^2\n","* where y = given y, (ground truth)\n","* $\\hat{y}$ = prediction\n","* $E[\\hat{y}]$ = expected value or average of predictions\n","* error = $\\frac{fp + fn}{total}$\n","* Bias = error in training\n","* Variance = range of predictions, how much an estimate of the target function will change if different data was is\n","\n","<table width='50%' style='margin-right: 100%'>\n","    <tr>\n","        <td>Training Error</td>\n","        <td>1%</td>\n","        <td>15%</td>\n","        <td>16%</td>\n","        <td>1%</td>\n","    </tr>\n","    <tr>\n","        <td>Testing Error</td>\n","        <td>15%</td>\n","        <td>16%</td>\n","        <td>33%</td>\n","        <td>1%</td>\n","    </tr>\n","    <tr>\n","        <td></td>\n","        <td>lo bias</td>\n","        <td>hi bias</td>\n","        <td>hi bias</td>\n","        <td>lo bias</td>\n","    </tr>\n","    <tr>\n","        <td></td>\n","        <td>hi var</td>\n","        <td>lo var</td>\n","        <td>hi var</td>\n","        <td>lo var</td>\n","    </tr>\n","</table>\n","\n","https://www.mastersindatascience.org/learning/difference-between-bias-and-variance/"]},{"cell_type":"markdown","id":"8b04dcf2","metadata":{"id":"8b04dcf2"},"source":["### Precision Recall Tradeoff\n","\n","In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance... Accuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score.\n","\n","https://en.wikipedia.org/wiki/Precision_and_recall\n","\n","In cases of imbalanced data, precision, recall, and f1 score become useful metrics. Precision tells us the proportion of data that was predicted was actually predicted correctly. Recall, also known as the True Positive Rate, is the probability that an actual positive will be predicted to be positive.\n","\n","Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\n","\n","https://en.wikipedia.org/wiki/Precision_and_recall\n","\n","<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/1*GM10sFuFBAbhwBt2scuP1g.png' alt='precision recall tradeoff' />\n","\n","https://datascience-george.medium.com/the-precision-recall-trade-off-aa295faba140\n","\n","The following is a visualization of our true negatives, false positives, false negatives, and true positives from the data set generated with make_classification."]},{"cell_type":"markdown","id":"11e9c355","metadata":{"id":"11e9c355"},"source":["## Metrics\n","\n","* tn = pred 0 actual 0\n","* fp = pred 1 actual 0\n","* fn = pred 0 actual 1\n","* tp = pred 1 actual 1\n","* acc(uracy) = $\\frac{tn + tp}{total}$\n","* error = $\\frac{fp + fn}{total}$\n","* prev(alence) = $\\frac{fn + tp}{total}$\n","* queue = $\\frac{fp + tp}{total}$\n","* tpr = $\\frac{tp}{tp + fn}$\n","    * true positive rate\n","    * recall\n","    * sensitivity\n","    * prob of detection\n","    * 1 - fnr\n","* fnr = $\\frac{fn}{tp + fn}$\n","    * false negative rate\n","    * type II error\n","    * 1 - tpr\n","* tnr = $\\frac{tn}{tn + fp}$\n","    * true negative rate\n","    * specificity\n","    * 1 - fpr\n","* fpr = $\\frac{fp}{tn + fp}$\n","    * false positive rate\n","    * type I error\n","    * fall out\n","    * prob of false claim\n","    * 1 - tnr\n","* ppv = $\\frac{tp}{tp + fp}$\n","    * positive predicted value\n","    * precision\n","    * 1 - fdr\n","* fdr = $\\frac{fp}{tp + fp}$\n","    * false discovery rate\n","    * 1 - ppv\n","* npv = $\\frac{tn}{tn + fn}$\n","    * negative predicted value\n","    * 1 - for\n","* for = $\\frac{fn}{tn + fn}$\n","    * false omission rate\n","    * 1 - npv\n","* liklihood ratio+ (lr+) = $\\frac{tpr}{fpr}$\n","    * roc\n","* liklihood ratio- (lr-) = $\\frac{fnr}{tnr}$\n","* diagnostic odds ratio = $\\frac{lr+}{lr-}$\n","* f1 score = 2 * $\\frac{precision-recall}{precision+recall}$\n","* Youden's J = sensitivity + specificity - 1 = tpr - fpr\n","* Matthew's Correlation Coefficient = $\\frac{(tp*tn)-(fp*tp)}{\\sqrt{(tp+fp)(tp+fn)(tn+fp)(tn+fn)}}$\n","  "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}