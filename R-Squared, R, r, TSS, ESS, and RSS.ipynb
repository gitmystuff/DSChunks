{"cells":[{"cell_type":"markdown","metadata":{"id":"QVEAMzZfTEeu"},"source":["# R-Squared, R, r, TSS, ESS, and RSS\n","\n","## R Squared\n","\n","**Traditional**\n","\n","* $SS_{res} = \\sum{(y - \\hat{y})^2}$\n","* $SS_{tot} = \\sum{(y - \\bar{y})^2}$\n","* $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n","\n","**Out Of Sample (OOS)**\n","\n","* $SS_{res} = \\sum{(y - \\hat{y})^2}$\n","* $SS_{tot} = \\sum{(y - \\bar{y}_{train})^2}$\n","* $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n","\n","https://towardsdatascience.com/whats-wrong-with-r-squared-and-how-to-fix-it-7362c5f26c53\n","\n","In statistics, the Pearson correlation coefficient ― also known as Pearson's r ― is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationship or correlation. As a simple example, one would expect the age and height of a sample of teenagers from a high school to have a Pearson correlation coefficient significantly greater than 0, but less than 1 (as 1 would represent an unrealistically perfect correlation).\n","\n","https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n","\n","In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\n","\n","https://en.wikipedia.org/wiki/Coefficient_of_determination\n","\n","* r shows correlation between x and y\n","* r squared shows strength of model, the proportion of the variance y that can be explained by X in a linear regression model\n","\n","## R Squared, R, and r\n","\n","* R-Squared: Coefficient of Determination, proportion of the variation in the dependent variable that is predictable from the independent variable(s)\n","* R: Coefficient of multiple correlation, a measure of how well a given variable can be predicted using a linear function of a set of other variables\n","* r: Pearson's r, Correlation is different from regression, as it does not assume any sort of dependency between two quantitative variables and it is only meant to express their joint variability\n","* https://www.r-bloggers.com/2022/11/the-coefficient-of-determination-is-it-the-r-squared-or-r-squared/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cwJ8seaqTEex"},"source":["### According to Bard\n","\n","R-squared and r are both measures of the strength of a linear relationship, but they measure different things. R-squared measures the proportion of the variance of the dependent variable that is explained by the independent variable. R measures the strength of the linear relationship between the two variables.\n","\n","In simple linear regression, R is the square root of R-squared. So, if R-squared is 0.50, then R is 0.71.\n","\n","In multiple linear regression, R is calculated using the same formula as in simple linear regression, but R-squared is calculated using a different formula.\n","\n","Here are some examples of how R-squared and r can be used:\n","\n","* If you have a data set that shows a strong linear relationship between two variables, you can use R-squared to estimate how much of the variation in the dependent variable can be explained by the variation in the independent variable.\n","* If you have a data set that shows a weak linear relationship between two variables, you can use r to estimate how strong the linear relationship is.\n","* You can also use R-squared to compare the strength of two different linear relationships. For example, if you have two data sets that show linear relationships between two variables, you can compare the R-squared values of the two data sets to see which one has a stronger linear relationship.\n","* It is important to note that R-squared and r are not perfect measures. They can be affected by outliers and other factors. However, they can be useful tools for estimating the strength of linear relationships."]},{"cell_type":"markdown","metadata":{"id":"C6hUqPfQTEey"},"source":["## The Rs with Simple Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rykF1pyTEey","outputId":"3185d4fc-9e65-469c-8ebe-f43cab54944b"},"outputs":[{"name":"stdout","output_type":"stream","text":["r with df.corr()            Feature 1         y\n","Feature 1   1.000000  0.987579\n","y           0.987579  1.000000\n","r with scipy 0.9875791397513904\n","r^2 0.9753125572720964\n","R squared (model2 score):  0.975312557272096\n","R squared (model score with X_train and y_train):  0.97584774237287\n","R squared (metric from sklearn of y_test and y_hat) 0.9735012189411446\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from scipy.stats import pearsonr\n","\n","# create dataframe\n","X, y = make_regression(n_samples=1000, n_features=1, noise=13)\n","df = pd.DataFrame(data=X, columns=['Feature 1'])\n","df['y'] = y\n","\n","# train test split; change the random_state to 42 if not using the generated dataset\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df['y'], test_size=.25, random_state=42)\n","\n","# create and train the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# create and train the model\n","model2 = LinearRegression()\n","model2.fit(X, y)\n","\n","# test set prediction results\n","y_hat = model.predict(X_test)\n","print('r with df.corr()', df.corr())\n","pcorr, _ = pearsonr(X.flatten(), y)\n","print('r with scipy', pcorr)\n","print('r^2', pcorr**2)\n","print('R squared (model2 score): ', model2.score(X, y))\n","print('R squared (model score with X_train and y_train): ', model.score(X_train, y_train))\n","print('R squared (metric from sklearn of y_test and y_hat)', r2_score(y_test, y_hat))"]},{"cell_type":"markdown","metadata":{"id":"EFnkdAnmTEe0"},"source":["## The Rs with Multiple Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H_fcfqdTEe1","outputId":"364cc14d-46e3-4a72-b16d-0bc2d87bb92b"},"outputs":[{"name":"stdout","output_type":"stream","text":["R squared (model score):  0.9657387189006659\n","R squared (metric from sklearn of y_test and y_hat) 0.9592641536685931\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","\n","# create dataframe\n","X, y = make_regression(n_samples=1000, n_features=3, noise=13)\n","df = pd.DataFrame(data=X, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n","df['y'] = y\n","\n","# train test split; change the random_state to 42 if not using the generated dataset\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df['y'], test_size=.25, random_state=42)\n","\n","# create and train the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# test set prediction results\n","y_hat = model.predict(X_test)\n","print('R squared (model score): ', model.score(X_train, y_train))\n","print('R squared (metric from sklearn of y_test and y_hat)', r2_score(y_test, y_hat))"]},{"cell_type":"markdown","source":["## Adjusted R-Squared\n","\n","Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n","\n","https://www.investopedia.com/ask/answers/012615/whats-difference-between-rsquared-and-adjusted-rsquared.asp"],"metadata":{"id":"entpzGRcTYuO"}},{"cell_type":"markdown","metadata":{"id":"-mUwx4ApTEe1"},"source":["## Total Sum of Squares\n","\n","According to Wikipedia, if $\\bar{y}$ is the mean of the observed data:\n","\n","$\n","\\bar{y} = \\frac{1}{N}\\sum(y)\n","$\n","\n","then the variability of the data set can be measured with two sums of squares formulas:\n","\n","The total sum of squares (proportional to the variance of the data):\n","\n","$\n","SS_{tot} = \\sum(y - \\bar{y})^2\n","$\n","\n","## Residual Sum of Squares\n","\n","The sum of squares of residuals, also called the residual sum of squares:\n","\n","$\n","SS_{res} = \\sum(y - \\hat{y})^2\n","$\n","\n","$\\hat{y}$ represents our predicted y.\n","\n","With this information, we can get r-squared:\n","\n","$\n","R^2 = 1 - \\large{\\frac{SS_{res}}{SS_{tot}}}\n","$\n","\n","https://en.wikipedia.org/wiki/Residual_sum_of_squares\n","\n","## Explained Sum of Squares\n","\n","Another sum of squares not mentioned is Explained Sum of Squares:\n","\n","$\n","ESS = \\sum(\\hat{y} - \\bar{y})^2\n","$\n","\n","or\n","\n","$\n","TSS = RSS + ESS\n","$\n","\n","or\n","\n","$\n","ESS = TSS - RSS\n","$\n","\n","https://en.wikipedia.org/wiki/Explained_sum_of_squares\n","\n","The formula for Adjusted R-Squared is:\n","\n","$\n","R^2_{adj} = 1 - (1 - R^2)\\large{\\frac{n-1}{n - p - 1}}\n","$\n","\n","or\n","\n","$\n","R^2_{adj} = 1 - \\large{\\frac{\\frac{SS_{res}}{df_e}}{\\frac{SS_{tot}}{df_t}}}\n","$\n","\n","where $df_t$ is the degrees of freedom n – 1 of the estimate of the population variance of the dependent variable, and $df_e$ is the degrees of freedom n – p – 1 of the estimate of the underlying population error variance. Note: p = parameters or features; n = observations\n","\n","The adjusted R2 can be negative, and its value will always be less than or equal to that of R2. Unlike R2, the adjusted R2 increases only when the increase in R2 (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance. If a set of explanatory variables with a predetermined hierarchy of importance are introduced into a regression one at a time, with the adjusted R2 computed each time, the level at which adjusted R2 reaches a maximum, and decreases afterward, would be the regression with the ideal combination of having the best fit without excess/unnecessary terms.\n","\n","https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}