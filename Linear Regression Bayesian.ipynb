{"cells":[{"cell_type":"markdown","id":"c0ae2bc0","metadata":{"id":"c0ae2bc0"},"source":["# Linear Regression Bayesian\n","\n","Why Bayes' Theorem?\n","\n","* David Hume (agnostic) miracles (Jesus resurrection) are violations of the laws of nature\n","* Bayes (reverend) responded by trying to quantify the probability of an event, Jesus resurrection\n","* Hume underestimated the impact of there being a number of independent witnesses to a miracle, and the Bayes' results showed how the muliplication of even fallible evidence could overwhelm the great improbability of an event and establish a fact\n","* 1700s Bayes wanted to know how to infer caused from effects\n","* vs David Hume's challenge that any cause-and-effect relationship is incorrect because thoughts are subjective and therefore causality cannot be proven\n","* Allows us to update our beliefs based on new evidence\n","* Extension of conditional probability\n","\n","Advantages\n","\n","* Incorporates uncertainty\n","* Makes use of distributions vs single point estimate\n","* Thus recovering a distribution of inferential solutions\n","\n","Disadvantages\n","\n","* The prior\n","\n","### Frequentists vs Bayesians\n","\n","* Frequentist: after enought flips, we can see the likelihood of an event occurring in the long run based on past frequency counts, remembering that for a frequentist, if you flip a coin 7 times out of 10, the probability is 70%\n","* Bayesian: The coin represents a random variable and therefore has a probability distribution\n","* Frequentist: Certainty\n","* Bayesian: Uncertainty\n","* Frequentist: It's either 100% heads or 100% tails\n","* Bayesian: There's a 50% probability\n","* Frequentist: count and frequencies\n","* Bayesian: beliefs and probability is based on assumptions"]},{"cell_type":"markdown","id":"aabfdd49","metadata":{"id":"aabfdd49"},"source":["Sources\n","\n","* https://faculty.som.yale.edu/jameschoi/bayes-theorem-began-as-a-defense-of-christianity/\n","* https://stats.stackexchange.com/questions/31867/bayesian-vs-frequentist-interpretations-of-probability\n","* https://medium.com/@sthacruz/frequentist-and-bayesian-approaches-to-interpreting-probability-and-making-decisions-based-on-data-8c4ad5891272\n","* Bayesian Linear Regression in Python from https://deeplearningcourses.com\n","* https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7\n","* https://youtu.be/Z6HGJMUakmc?si=1ONAsFQ7O82SL4W3\n","* https://youtu.be/LzZ5b3wdZQk?si=ed305eTsXvEwEBVH\n","* Random variables have a distribution"]},{"cell_type":"markdown","id":"a29c6433","metadata":{"id":"a29c6433"},"source":["## Intersections\n","\n","$A \\cap B = \\{\\: x: x \\in A \\: and \\: x \\in B \\:\\}$\n","\n","order doesn't matter $A \\cap B$ or $B \\cap A$\n","\n","### Conditional Probability\n","\n","$P(A \\cap B) = P(B \\cap A) = P(B) * P(A|B) = P(A) * P(B|A)$\n","\n","### Bayes' Theorem\n","\n","$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)}$\n","\n","or\n","\n","$Posterior = \\frac{Prior * Likelihood}{Marginal Likelihood (Evidence)}$"]},{"cell_type":"markdown","id":"bf4ca6a8","metadata":{"id":"bf4ca6a8"},"source":["## The Model\n","\n","$y = \\theta x + \\epsilon$\n","\n","* where $\\epsilon \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)$\n","* and $\\epsilon$ is model noise and generally unknown\n","* vs $(y - \\hat{y})$ is residual (actual y minus observed y)\n","\n","Then it is given\n","\n","* $\\hat{\\theta} \\sim \\mathcal{N}(\\theta, \\frac{\\sigma^2}{\\sum{x^2}})$\n","* Variance decreases with more data\n","* Variance is proportional to measurement noise\n","* The sums of independent Normals is Normal"]},{"cell_type":"markdown","id":"0d8e4160","metadata":{"id":"0d8e4160"},"source":["## MLE\n","\n","* Maximum Likelihood Estimation\n","* Minimizing SSE is equivalent to MLE\n","* $p(data | params)$\n","* $p(y | X, \\theta) = \\prod p(y|x,\\theta)$\n","* $\\mathcal{L}(\\theta) = p(y | X, \\theta) = \\prod \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\large{e^\\frac{-(y-\\theta x)^2}{2\\sigma^2}}$"]},{"cell_type":"markdown","id":"1d2bbff6","metadata":{"id":"1d2bbff6"},"source":["## Log-Likelihood\n","\n","* Logs are computational power friendly\n","* $log \\mathcal{L} = N log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) - \\frac{1}{2\\sigma^2}\\sum (y-\\theta x)^2$"]},{"cell_type":"markdown","id":"9a21e102","metadata":{"id":"9a21e102"},"source":["## MAP\n","\n","* Maximum A Posteriori Estimation\n","* Equivalent to regularization; prevents overfitting, improves generalization\n","* The prior $p(\\theta) \\sim \\mathcal{N}(0, \\lambda^{-1})$\n","* Lambda = precision, the inverse of variance (variance = $1/\\lambda$)\n","* Recall $p(\\theta | data) = \\frac{p(data | \\theta)p(\\theta)}{p(data)}$\n","* $\\hat{\\theta} = argmax \\frac{p(data | \\theta)p(\\theta)}{p(data)}$\n","* $E[\\hat{\\theta}]_{map} = \\theta \\frac{\\sum{x^2}}{\\sigma^2 \\lambda + \\sum{x^2}}$"]},{"cell_type":"markdown","id":"7f81cdab","metadata":{"id":"7f81cdab"},"source":["## Ridge Regularization\n","\n","* $\\sum(y - \\sum\\theta x)^2 + \\lambda \\sum \\theta^2$"]},{"cell_type":"markdown","id":"81d47f45","metadata":{"id":"81d47f45"},"source":["## Estimating $\\theta$\n","\n","E[$\\hat{\\theta}$] = $\\frac{\\sum{xy}}{\\sum{x}^2}$"]},{"cell_type":"code","execution_count":null,"id":"5edbbbb3","metadata":{"id":"5edbbbb3","outputId":"398649fa-3899-467b-fef2-b5da75a52a09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Make Coef: 5.8083612168199465\n","Estimate: 5.722699825988811\n","Closed Form: 5.689306829342066\n","\n","Make Coef: 14.092422497476264\n","Estimate: 14.186886823359409\n","Closed Form: 14.226942903131034\n","\n","Make Coef: 64.59172413316013\n","Estimate: 64.53521862990064\n","Closed Form: 64.5354973965827\n","\n","Make Coef: 16.82365791084919\n","Estimate: 16.820428783580248\n","Closed Form: 16.82054540152137\n","\n","Make Coef: 0.27705053658392265\n","Estimate: 0.2854728100692889\n","Closed Form: 0.2854715709713808\n","\n"]}],"source":["import numpy as np\n","import scipy.stats as stats\n","from sklearn.datasets import make_regression\n","\n","for n in [5, 50, 500, 5000, 50000]:\n","    X, y, coef = make_regression(n_samples=n, n_features=1, noise=1, coef=True, random_state=42)\n","    print('Make Coef:', coef)\n","    print('Estimate:', np.sum([x*y for x, y in zip(X, y)])/np.sum([x**2 for x in X]))\n","    x = X.flatten()\n","    x = np.vstack([np.ones(len(x)), x]).T\n","    a, b = np.linalg.lstsq(x, y, rcond=None)[0]\n","    print('Closed Form:', b)\n","    print()"]},{"cell_type":"markdown","id":"d3fcacd7","metadata":{"id":"d3fcacd7"},"source":["### Regularization vs Bayes Approach\n","\n","https://youtu.be/Z6HGJMUakmc?si=1ONAsFQ7O82SL4W3\n","\n","* $\\lambda$ (given the prior such as in Ridge)\n","* $\\lambda = (\\sigma^2)^{-1}$ for Bayesian Regression"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}