{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PrePy\n","\n","Data preparation example"],"metadata":{"id":"uFY2z81gIEB6"}},{"cell_type":"markdown","source":["## Cardinality and Constants\n","\n","**Code Explanation**\n","\n","```python\n","constant_features = [\n","    feat for feat in df.columns if len(df[feat].unique()) == 1\n","]\n","```\n","\n","This Python code snippet uses a list comprehension to identify columns in a Pandas DataFrame (`df`) that contain only a single unique value. Here's a step-by-step breakdown:\n","\n","1.  **`df.columns`**: This retrieves a list of all column names from the DataFrame `df`.\n","2.  **`for feat in df.columns`**: This iterates through each column name, assigning the current column name to the variable `feat`.\n","3.  **`df[feat].unique()`**: For each column (`feat`), this extracts an array of the unique values present in that column.\n","4.  **`len(df[feat].unique()) == 1`**: This calculates the number of unique values in the column and checks if it's equal to 1. If it is, it means all values in that column are the same.\n","5.  **`[feat ... ]`**: If the condition in step 4 is true, the column name (`feat`) is added to the `constant_features` list.\n","\n","**In essence, this code identifies columns that are \"constant\" – they don't provide any variability or information because they have only one value.**\n","\n","**Why Cardinality and Constants Are Important in Data Modeling**\n","\n","Now, let's discuss why cardinality and constants are crucial considerations in data modeling:\n","\n","**1. Constants**\n","\n","* **Redundancy and Noise:** Constant features add no predictive power to a model. They introduce redundancy and can even be considered noise, potentially slowing down training and affecting model performance.\n","* **Wasted Resources:** Storing and processing constant features wastes computational resources (memory, processing time) without providing any benefit.\n","* **Model Interpretation:** Including constant features can complicate model interpretation, as they don't contribute to understanding the relationships between variables.\n","* **Data Cleaning:** Identifying and removing constant features is an essential step in data cleaning and preprocessing.\n","\n","**2. Cardinality**\n","\n","Cardinality refers to the number of unique values in a categorical feature. It's important for several reasons:\n","\n","* **High Cardinality:**\n","    * Features with very high cardinality (e.g., unique IDs, zip codes) can lead to overfitting, as the model may memorize specific patterns related to individual values rather than learning generalizable relationships.\n","    * They can create a large number of dummy variables when one-hot encoded, increasing the dimensionality of the data and computational complexity.\n","    * They might not provide meaningful insights if the unique values are not inherently ordered or related to the target variable.\n","* **Low Cardinality:**\n","    * Features with low cardinality (e.g., gender, day of the week) are often more useful for modeling, as they represent distinct categories with potential relationships to the target variable.\n","    * They are easier to handle and interpret.\n","* **Memory and Performance:** High-cardinality categorical variables can consume significant memory and slow down processing, especially when working with large datasets.\n","* **Encoding Strategies:** The choice of encoding strategy (e.g., one-hot encoding, label encoding, target encoding) depends on the cardinality of the feature. High-cardinality features might require more sophisticated encoding techniques.\n","* **Feature Engineering:** Understanding cardinality helps in feature engineering. You can group or bin high-cardinality features to create lower-cardinality features that are more informative.\n","\n","**In summary:**\n","\n","* **Constants** should generally be removed to improve model efficiency and reduce noise.\n","* **Cardinality** needs to be carefully considered to avoid overfitting, manage computational resources, and choose appropriate encoding strategies.\n","\n","By addressing these issues, you can build more robust, efficient, and interpretable data models.\n"],"metadata":{"id":"F08VGmIFIiyW"}},{"cell_type":"code","source":["constant_features = [\n","    feat for feat in df.columns if len(df[feat].unique()) == 1\n","]\n"],"metadata":{"id":"TLeBU9HJIgq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Quasi Constants\n","\n","**Code Explanation**\n","\n","```python\n","quasi_consts = []\n","for val in df.columns.sort_values():\n","  if (len(df[val].unique()) < 3 and max(df[val].value_counts(normalize=True)) > thresh):\n","    quasi_consts.append(val)\n","```\n","\n","This Python code snippet identifies columns in a Pandas DataFrame (`df`) that are \"quasi-constant\" – meaning they have very few unique values, and one of those values occurs with a high frequency. Here's a detailed explanation:\n","\n","1.  **`quasi_consts = []`**: Initializes an empty list called `quasi_consts` to store the names of the identified quasi-constant columns.\n","2.  **`df.columns.sort_values()`**: Gets the column names of the DataFrame `df` and sorts them alphabetically. This sorting is often done for consistency but doesn't change the logic.\n","3.  **`for val in df.columns.sort_values()`**: Iterates through each sorted column name, assigning the current column name to the variable `val`.\n","4.  **`len(df[val].unique()) < 3`**: Checks if the number of unique values in the column `val` is less than 3. This means it's looking for columns with only 1 or 2 unique values.\n","5.  **`df[val].value_counts(normalize=True)`**: Calculates the relative frequency of each unique value in the column `val`. `normalize=True` ensures the counts are represented as proportions (probabilities).\n","6.  **`max(df[val].value_counts(normalize=True)) > thresh`**: Finds the maximum relative frequency (the proportion of the most frequent value) and checks if it's greater than a predefined threshold `thresh`.\n","7.  **`if (len(df[val].unique()) < 3 and max(df[val].value_counts(normalize=True)) > thresh)`**: Combines the conditions from steps 4 and 6. A column is considered quasi-constant if it has fewer than 3 unique values and the most frequent value exceeds the threshold.\n","8.  **`quasi_consts.append(val)`**: If both conditions are met, the column name `val` is appended to the `quasi_consts` list.\n","\n","**In essence, this code identifies columns where one value dominates, even if there's a second (or third) infrequent value.**\n","\n","**Why Quasi-Constants Are Important in Data Modeling**\n","\n","Quasi-constant features, while not entirely constant, can still pose challenges in data modeling:\n","\n","* **Low Information Content:** They provide very little variability and, therefore, limited information for the model to learn from. The model might overemphasize the dominant value, leading to biased predictions.\n","* **Potential for Overfitting:** If the infrequent values are noisy or outliers, the model might try to fit to them, leading to overfitting.\n","* **Skewed Distributions:** Quasi-constant features often result in highly skewed distributions, which can violate assumptions of some statistical models.\n","* **Feature Importance:** They might appear to be important simply because they're strongly correlated with the dominant outcome, but they might not represent a genuine causal relationship.\n","* **Computational Efficiency:** While not as severe as constant features, quasi-constants can still add unnecessary computational burden, especially in high-dimensional datasets.\n","* **Threshold Dependence:** The identification of quasi-constants depends on the chosen threshold (`thresh`). Selecting an appropriate threshold is critical and requires careful consideration of the data and modeling goals.\n","\n","**Practical Considerations**\n","\n","* Determining the appropriate `thresh` value is essential. A very high threshold might miss some quasi-constants, while a low threshold might remove features that provide useful information.\n","* The context of the data is crucial. A feature might be quasi-constant in one dataset but highly informative in another.\n","* When dropping quasi constant variables, you should take into account the effect that could have on the model.\n","* Sometimes, quasi-constant features can be transformed or binned to create more informative features.\n","\n","**In summary:**\n","\n","* Quasi-constants should be carefully evaluated because they can introduce bias and reduce model performance.\n","* The decision to remove or transform them depends on the specific dataset and modeling objectives.\n","* It is very important to test the model with and without these features.\n"],"metadata":{"id":"9UR8jg5VJe-L"}},{"cell_type":"code","source":["quasi_consts = []\n","for val in df.columns.sort_values():\n","  if (len(df[val].unique()) < 3 and max(df[val].value_counts(normalize=True)) > thresh):\n","    quasi_consts.append(val)"],"metadata":{"id":"UBkuQ_tYJfa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Duplicate Rows\n","\n","**Code Explanation**\n","\n","```python\n","len(df[df.duplicated(keep=False)])\n","```\n","\n","This Python code snippet, using the Pandas library, calculates the total number of duplicate rows within a DataFrame (`df`). Here's a step-by-step breakdown:\n","\n","1.  **`df.duplicated(keep=False)`**:\n","    * This Pandas method identifies duplicate rows within the DataFrame `df`.\n","    * The `keep=False` argument is crucial. It marks *all* duplicate rows as `True`, including the first occurrence. By default, `keep='first'` would only mark subsequent duplicates as `True`.\n","    * The result is a boolean Series (like a list) where `True` indicates a duplicate row and `False` indicates a unique row.\n","2.  **`df[df.duplicated(keep=False)]`**:\n","    * This uses boolean indexing to select the rows from the DataFrame `df` where the corresponding value in the boolean Series from step 1 is `True`. In other words, it creates a new DataFrame containing only the duplicate rows.\n","3.  **`len(...)`**:\n","    * This calculates the number of rows in the DataFrame resulting from step 2. This effectively counts the total number of duplicate rows in the original DataFrame `df`.\n","\n","**In essence, this code returns the total number of rows that are duplicates of at least one other row in the DataFrame.**\n","\n","**Why Addressing Duplicate Rows Is Important in Data Modeling**\n","\n","Duplicate rows can significantly impact data modeling and analysis in several ways:\n","\n","* **Bias and Misleading Statistics:**\n","    * Duplicates can skew statistical measures, such as means, medians, and standard deviations.\n","    * They can lead to an overestimation of the frequency of certain patterns or outcomes, resulting in biased models.\n","* **Overfitting:**\n","    * If a model is trained on data with duplicate rows, it may overfit to those specific duplicates, leading to poor generalization on unseen data.\n","    * The model might learn spurious patterns that are specific to the duplicated data, rather than generalizable relationships.\n","* **Reduced Model Performance:**\n","    * Duplicates can inflate the apparent size of the dataset without adding any new information, potentially slowing down training and reducing model efficiency.\n","* **Data Integrity Issues:**\n","    * Duplicate rows often indicate underlying data quality problems, such as errors in data collection, processing, or merging.\n","    * Addressing duplicates helps ensure data integrity and consistency.\n","* **Waste of Resources:**\n","    * Duplicate rows consume unnecessary memory and processing power.\n","* **Incorrect Evaluation:**\n","    * If a test set contains duplicate rows, it may lead to an overestimation of the models performance.\n","\n","**How to Handle Duplicate Rows**\n","\n","* **Identify Duplicates:** Use the `df.duplicated()` method to identify duplicate rows.\n","* **Remove Duplicates:** Use the `df.drop_duplicates()` method to remove duplicate rows. You can choose to keep the first or last occurrence of each duplicate or remove all of them.\n","* **Investigate the Source:** If duplicates are frequent, investigate the source of the data to identify and resolve the underlying data quality issues.\n","* **Aggregate Data:** In some cases, instead of removing duplicates, it might be appropriate to aggregate the data, such as calculating the mean or sum of relevant columns for each unique row.\n","\n","By addressing duplicate rows, you can improve the quality, reliability, and performance of your data models.\n"],"metadata":{"id":"GVMNygH9KNdC"}},{"cell_type":"code","source":["len(df[df.duplicated(keep=False)])"],"metadata":{"id":"JuMqnuAmKN0m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Duplicate Columns\n","\n","**Code Explanation**\n","\n","```python\n","duplicate_features = []\n","for i in range(0, len(df.columns)):\n","    orig = df.columns[i]\n","\n","    for dupe in df.columns[i + 1:]:\n","        if df[orig].equals(df[dupe]):\n","            duplicate_features.append(dupe)\n","```\n","\n","This Python code snippet, using Pandas, identifies and stores the names of duplicate columns within a DataFrame (`df`). Here's a step-by-step breakdown:\n","\n","1.  **`duplicate_features = []`**: Initializes an empty list called `duplicate_features` to store the names of the duplicate columns.\n","2.  **`for i in range(0, len(df.columns))`**: This outer loop iterates through each column in the DataFrame, using the index `i`.\n","3.  **`orig = df.columns[i]`**: Stores the name of the current column (the \"original\" column) being compared.\n","4.  **`for dupe in df.columns[i + 1:]`**: This inner loop iterates through the remaining columns in the DataFrame, starting from the column after the current `orig` column. This prevents comparing a column with itself and avoids redundant comparisons.\n","5.  **`if df[orig].equals(df[dupe])`**: This is the core logic. It checks if the values in the `orig` column are exactly equal to the values in the `dupe` column. The `equals()` method ensures a precise comparison of the entire Series.\n","6.  **`duplicate_features.append(dupe)`**: If the columns are equal (i.e., duplicates), the name of the duplicate column (`dupe`) is appended to the `duplicate_features` list.\n","\n","**In essence, this code finds columns that contain identical data and stores the name of the duplicate columns.**\n","\n","**Why Addressing Duplicate Columns Is Important in Data Modeling**\n","\n","Duplicate columns, like duplicate rows, can introduce problems in data modeling:\n","\n","* **Redundancy and Inefficiency:**\n","    * Duplicate columns provide redundant information, wasting storage space and increasing computational cost.\n","    * They don't add any new information to the model.\n","* **Multicollinearity:**\n","    * In statistical modeling (especially linear regression), duplicate columns represent perfect multicollinearity. This can lead to unstable and unreliable model coefficients.\n","    * It can make it difficult to determine the individual impact of each feature on the target variable.\n","* **Model Complexity:**\n","    * Duplicate columns can unnecessarily increase the complexity of the model, making it harder to interpret and understand.\n","* **Potential for Errors:**\n","    * If one of the duplicate columns is modified or updated, it could lead to inconsistencies if the other duplicate column is not updated accordingly.\n","* **Feature Importance Misinterpretation:**\n","    * If feature importance is calculated, both duplicate columns could appear important, when only one represents the information.\n","\n","**How to Handle Duplicate Columns**\n","\n","* **Identify Duplicates:** Use the code provided or similar methods to identify duplicate columns.\n","* **Remove Duplicates:** Use the `df.drop()` method to remove the duplicate columns. You should choose which column to keep based on its relevance, completeness, or ease of interpretation.\n","* **Investigate the Source:** If duplicate columns are frequent, investigate the data source to determine why they exist and prevent their creation in the future.\n","* **Feature Engineering:** Sometimes, the duplicate columns can be used to create new, more informative features.\n","\n","By addressing duplicate columns, you can improve the efficiency, stability, and interpretability of your data models.\n"],"metadata":{"id":"_qyxKttLLKim"}},{"cell_type":"code","source":["duplicate_features = []\n","for i in range(0, len(df.columns)):\n","  orig = df.columns[i]\n","\n","  for dupe in df.columns[i + 1:]:\n","    if df[orig].equals(df[dupe]):\n","        duplicate_features.append(dupe)"],"metadata":{"id":"j8DUmQoZLK7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Category Encoding\n","\n","**Code Explanation**\n","\n","This code snippet performs categorical feature encoding on a Pandas DataFrame (`df`) using a combination of techniques based on the cardinality (number of unique values) of the categorical features.\n","\n","1.  **Identifying Categorical Features:**\n","\n","    ```python\n","    cat_features = []\n","    for feat in df.select_dtypes(include=['object', 'category']):\n","    ```\n","\n","    * This part identifies columns in the DataFrame that have data types 'object' (strings) or 'category'. These are typically categorical features.\n","\n","2.  **Binary Encoding (Cardinality < 3):**\n","\n","    ```python\n","    if len(df[feat].value_counts()) < 3:\n","        df[feat] = df[feat].map({df[feat].value_counts().index[0]: 0, df[feat].value_counts().index[1]: 1})\n","        df[feat] = df[feat].astype(int)\n","    ```\n","\n","    * If a categorical feature has only two unique values (binary), it's encoded using a simple mapping: the most frequent value is mapped to 0, and the second most frequent to 1.\n","    * The column is then cast to integer type.\n","\n","3.  **Storing Medium Cardinality Features (2 < Cardinality < 6):**\n","\n","    ```python\n","    elif 2 < len(df[feat].value_counts()) < 6:\n","        cat_features.append(feat)\n","    ```\n","\n","    * If a categorical feature has 3 to 5 unique values, its name is added to the `cat_features` list. These features will later be one-hot encoded.\n","\n","4.  **Frequency Encoding (Cardinality > 5):**\n","\n","    ```python\n","    elif len(df[feat].value_counts()) > 5:\n","        freq = df.groupby(feat, observed=False).size()/len(df)\n","        df[feat] = df[feat].map(freq)\n","    ```\n","\n","    * If a categorical feature has more than 5 unique values (high cardinality), it's frequency encoded.\n","    * `df.groupby(feat).size()/len(df)` calculates the frequency of each unique value in the column.\n","    * `df[feat].map(freq)` replaces each value in the column with its corresponding frequency.\n","\n","5.  **One-Hot Encoding:**\n","\n","    ```python\n","    ohe = OneHotEncoder(categories='auto', drop='first', sparse_output=False, handle_unknown='ignore')\n","    ohe_df = ohe.fit_transform(df[cat_features])\n","    ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out(cat_features))\n","    df.index = df.index\n","    df = df.join(ohe_df)\n","    df.drop(cat_features, axis=1, inplace=True)\n","    ```\n","\n","    * `OneHotEncoder` from scikit-learn is used to one-hot encode the features in `cat_features`.\n","    * `drop='first'` removes the first category of each feature to avoid multicollinearity.\n","    * `sparse_output=False` returns a dense numpy array instead of a sparse matrix.\n","    * `handle_unknown='ignore'` ignores unknown categories that may appear in the test set.\n","    * The encoded features are converted into a new DataFrame (`ohe_df`).\n","    * The new one hot encoded dataframe is joined to the original dataframe.\n","    * The original categorical columns are dropped.\n","\n","**Why Categorical Encoding Is Important in Data Modeling**\n","\n","* **Machine Learning Algorithms:** Most machine learning algorithms work with numerical data. Categorical features need to be converted into numerical representations before they can be used in these algorithms.\n","* **Preventing Misinterpretation:** Without encoding, the algorithm might interpret categorical values as having a numerical order or ranking, which is often incorrect.\n","* **Improving Model Performance:** Proper encoding can significantly improve model performance by providing meaningful numerical representations of categorical data.\n","* **Handling Different Cardinality:** Different encoding techniques are suitable for different cardinalities.\n","    * Binary encoding is efficient for features with two categories.\n","    * One-hot encoding is suitable for low-to-medium cardinality features.\n","    * Frequency encoding is appropriate for high-cardinality features.\n","* **Avoiding Multicollinearity:** Techniques like dropping the first category in one-hot encoding help prevent multicollinearity, which can negatively affect model stability.\n","* **Feature Engineering:** Categorical encoding is a crucial part of feature engineering, allowing you to transform raw categorical data into features that are more informative for the model.\n","\n","In summary, categorical encoding is a fundamental step in data preprocessing for machine learning. It ensures that categorical data is represented in a way that algorithms can understand and use effectively, leading to better model performance and more accurate predictions.\n"],"metadata":{"id":"F0kYGp1UL-sA"}},{"cell_type":"code","source":["cat_features = []\n","for feat in df.select_dtypes(include=['object', 'category']):\n","  if len(df[feat].value_counts()) < 3:\n","    df[feat] = df[feat].map({df[feat].value_counts().index[0]: 0, df[feat].value_counts().index[1]: 1})\n","    df[feat] = df[feat].astype(int)\n","  elif 2 < len(df[feat].value_counts()) < 6:\n","    cat_features.append(feat)\n","  elif len(df[feat].value_counts()) > 5:\n","    freq = df.groupby(feat, observed=False).size()/len(df)\n","    df[feat] = df[feat].map(freq)\n","\n","ohe = OneHotEncoder(categories='auto', drop='first', sparse_output=False, handle_unknown='ignore')\n","ohe_df = ohe.fit_transform(df[cat_features])\n","ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out(cat_features))\n","df.index = df.index\n","df = df.join(ohe_df)\n","df.drop(cat_features, axis=1, inplace=True)"],"metadata":{"id":"0bjamQFIL_K6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Missing Values\n","\n","**Code Explanation**\n","\n","This code snippet addresses missing values in a Pandas DataFrame (`df`) using different strategies based on the feature's data type and skewness.\n","\n","1.  **Identifying Categorical Features:**\n","\n","    ```python\n","    df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n","    dfx = df.copy()\n","    ```\n","\n","    * `df_categorical_features` stores the names of columns with 'category' or 'object' data types (categorical features).\n","    * `dfx = df.copy()` creates a copy of the original DataFrame to avoid modifying it directly.\n","\n","2.  **Iterating Through Columns with Missing Values:**\n","\n","    ```python\n","    for feat in df.columns[df.isnull().sum() > 1]:\n","    ```\n","\n","    * This loop iterates through columns that have more than one missing value. `df.isnull().sum() > 1` identifies such columns.\n","\n","3.  **Handling Categorical Features:**\n","\n","    ```python\n","    if feat in df_categorical_features:\n","        dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n","    ```\n","\n","    * If the current column (`feat`) is categorical, missing values are filled with the mode (most frequent value) of that column. `df[feat].mode()[0]` gets the mode.\n","\n","4.  **Handling Numerical Features (Based on Skewness):**\n","\n","    ```python\n","    else:\n","        if abs(df[feat].skew()) < .8:\n","            dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n","        else:\n","            dfx[feat] = df[feat].fillna(df[feat].median())\n","    ```\n","\n","    * If the column is numerical, the handling depends on its skewness:\n","        * If the absolute skewness is less than 0.8 (relatively symmetric distribution), missing values are filled with the mean, rounded to two decimal places.\n","        * If the absolute skewness is 0.8 or greater (skewed distribution), missing values are filled with the median.\n","\n","**Why Addressing Missing Values Is Important in Data Modeling**\n","\n","Missing values are a common problem in real-world datasets and can significantly impact data modeling in several ways:\n","\n","* **Algorithm Compatibility:** Many machine learning algorithms cannot handle missing values. Trying to train a model on data with missing values can lead to errors or unexpected behavior.\n","* **Bias and Distortion:** Missing values can introduce bias into the data if they are not randomly distributed. Filling missing values with inappropriate values can distort the underlying relationships between variables.\n","* **Reduced Model Performance:** Missing values can reduce the amount of information available to the model, leading to lower accuracy and poorer generalization.\n","* **Data Integrity Issues:** Missing values can indicate underlying data quality problems, such as errors in data collection, processing, or storage.\n","* **Statistical Analysis:** Missing values can complicate statistical analysis and make it difficult to draw valid conclusions from the data.\n","* **Increased Model Complexity:** Some methods of dealing with missing data, like creating dummy variables for missing values, can increase the dimensionality of the data, which can negatively impact model performance.\n","\n","**Why the Specific Filling Methods**\n","\n","* **Mode for Categorical Features:** The mode is a suitable fill value for categorical features because it represents the most common category.\n","* **Mean for Symmetrical Numerical Features:** The mean is a good choice for filling missing values in symmetrical distributions because it represents the central tendency of the data. Rounding the mean to two decimal places is a common practice to avoid introducing unnecessary precision.\n","* **Median for Skewed Numerical Features:** The median is more robust to outliers than the mean and is, therefore, a better choice for skewed distributions.\n","\n","**In summary:**\n","\n","* Addressing missing values is crucial for building accurate and reliable data models.\n","* The choice of filling method should depend on the data type and distribution of the features.\n","* It is very important to document how and why missing values were handled.\n"],"metadata":{"id":"lV5taJoSMxg8"}},{"cell_type":"code","source":["df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n","dfx = df.copy()\n","for feat in df.columns[df.isnull().sum() > 1]:\n","  if feat in df_categorical_features:\n","    dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n","  else:\n","    if abs(df[feat].skew()) < .8:\n","      dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n","    else:\n","      dfx[feat] = df[feat].fillna(df[feat].median())"],"metadata":{"id":"6Po-xaikMzgU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Standardized Scaling\n","\n","**Code Explanation**\n","\n","```python\n","feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = StandardScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)\n","```\n","\n","This code snippet performs standardized scaling on a single numerical column within a Pandas DataFrame (`df`). Here's a step-by-step explanation:\n","\n","1.  **`feat = str(df._get_numeric_data().idxmax(1)[0])`**:\n","    * `df._get_numeric_data()` selects only the numerical columns from the DataFrame.\n","    * `.idxmax(1)` finds the index (column name) of the maximum value in each row. Since we are only interested in a single column, we will be getting the first column name.\n","    * `[0]` extracts the first column name from the result.\n","    * `str()` converts the column name to a string, ensuring it can be used for indexing.\n","    * In short, this line selects the name of the first numerical column in the dataframe.\n","\n","2.  **`scaler = StandardScaler()`**:\n","    * Creates an instance of the `StandardScaler` from scikit-learn. This scaler will standardize features by removing the mean and scaling to unit variance.\n","\n","3.  **`df[feat] = scaler.fit_transform(df[[feat]].values)`**:\n","    * `df[[feat]].values` selects the specified column (`feat`) as a DataFrame (to maintain dimensionality) and converts it to a NumPy array.\n","    * `scaler.fit_transform(...)` fits the scaler to the data (calculates the mean and standard deviation) and then transforms the data (standardizes it).\n","    * `df[feat] = ...` replaces the original column in the DataFrame with the standardized values.\n","\n","**Why Standardized Scaling Is Important in Data Modeling**\n","\n","Standardized scaling (also known as Z-score normalization) is crucial for several reasons:\n","\n","* **Algorithm Sensitivity:** Many machine learning algorithms are sensitive to the scale of input features. Features with larger ranges can dominate those with smaller ranges, leading to biased or suboptimal model performance.\n","* **Gradient Descent Optimization:** Gradient descent-based algorithms (e.g., linear regression, logistic regression, neural networks) converge faster when features are on a similar scale.\n","* **Distance-Based Algorithms:** Distance-based algorithms (e.g., k-nearest neighbors, k-means clustering) are highly sensitive to feature scales. Standardizing features ensures that distances are calculated fairly.\n","* **Regularization:** Regularization techniques (e.g., L1, L2 regularization) can be affected by feature scales. Standardizing features helps ensure that regularization is applied consistently.\n","* **Improving Model Stability:** Standardizing features can improve the numerical stability of some algorithms.\n","\n","**When to Use Standardized Scaling**\n","\n","Standardized scaling is generally recommended in these situations:\n","\n","* **Algorithms Sensitive to Scale:** When using algorithms like:\n","    * Linear regression\n","    * Logistic regression\n","    * Support vector machines (SVMs)\n","    * Neural networks\n","    * K-nearest neighbors (KNN)\n","    * Principal component analysis (PCA)\n","* **Features with Gaussian-Like Distributions:** Standardized scaling works best when features have a roughly Gaussian (normal) distribution.\n","* **When Feature Ranges Vary Significantly:** If features have vastly different ranges (e.g., one feature ranges from 0 to 1, and another ranges from -1000 to 1000), standardized scaling is essential.\n","* **When Outliers Are Not a Major Concern:** Standardized scaling is sensitive to outliers. If your data contains significant outliers, robust scaling methods (e.g., `RobustScaler`) might be more appropriate.\n","\n","**Key Points About Standardized Scaling**\n","\n","* Transforms data to have a mean of 0 and a standard deviation of 1.\n","* Can be affected by outliers.\n","* Preserves the shape of the original distribution.\n","* Is a common and effective scaling technique for many machine learning algorithms.\n"],"metadata":{"id":"MAHhQcLfNjga"}},{"cell_type":"code","source":["feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = StandardScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)"],"metadata":{"id":"woxzEi1UNjQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MinMax Scaling\n","\n","**Code Explanation**\n","\n","```python\n","feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = MinMaxScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)\n","```\n","\n","This code snippet performs min-max scaling on a single numerical column within a Pandas DataFrame (`df`). Here's a step-by-step explanation:\n","\n","1.  **`feat = str(df._get_numeric_data().idxmax(1)[0])`**:\n","    * This line, as explained before, selects the first numerical column of the dataframe, and stores it as a string.\n","2.  **`scaler = MinMaxScaler()`**:\n","    * Creates an instance of the `MinMaxScaler` from scikit-learn. This scaler will transform features by scaling each feature to a given range, typically between 0 and 1.\n","3.  **`df[feat] = scaler.fit_transform(df[[feat]].values)`**:\n","    * `df[[feat]].values` selects the specified column (`feat`) as a DataFrame (to maintain dimensionality) and converts it to a NumPy array.\n","    * `scaler.fit_transform(...)` fits the scaler to the data (calculates the minimum and maximum values) and then transforms the data (scales it to the specified range).\n","    * `df[feat] = ...` replaces the original column in the DataFrame with the scaled values.\n","\n","**Why Min-Max Scaling Is Important in Data Modeling**\n","\n","Min-max scaling is essential for several reasons:\n","\n","* **Range Normalization:** It ensures that all features are within a specific range, typically 0 to 1, which can be crucial for algorithms that are sensitive to feature ranges.\n","* **Preventing Feature Domination:** It prevents features with larger ranges from dominating those with smaller ranges, leading to fairer and more accurate model performance.\n","* **Image Processing:** Min-max scaling is commonly used in image processing to normalize pixel values to a specific range (e.g., 0 to 1 or 0 to 255).\n","* **Neural Networks:** Some neural network activation functions (e.g., sigmoid) work best with input values in a specific range.\n","* **Distance-Based Algorithms:** While standardized scaling is often preferred, min-max scaling can also be used with distance-based algorithms (e.g., k-nearest neighbors) to ensure that distances are calculated fairly.\n","\n","**When to Use Min-Max Scaling**\n","\n","Min-max scaling is generally recommended in these situations:\n","\n","* **Algorithms Sensitive to Range:** When using algorithms that are sensitive to the range of input features, such as:\n","    * Neural networks (especially with certain activation functions)\n","    * Image processing algorithms\n","* **When You Need Data in a Specific Range:** If you need to transform your data to a specific range (e.g., 0 to 1), min-max scaling is the appropriate choice.\n","* **When You Know the Exact Minimum and Maximum Values:** If you know the exact minimum and maximum values of your data, min-max scaling can be more efficient than standardized scaling.\n","* **When Outliers Are Not a Major Concern:** Similar to standardized scaling, min-max scaling is sensitive to outliers. Outliers can significantly affect the minimum and maximum values, leading to skewed scaling.\n","* **When distributions are not Gaussian:** Min max scaling does not change the distribution of the data, and is therefore useful when the data is not normally distributed.\n","\n","**Key Points About Min-Max Scaling**\n","\n","* Transforms data to a specific range (typically 0 to 1).\n","* Sensitive to outliers.\n","* Preserves the shape of the original distribution.\n","* Is useful when a fixed range is required.\n"],"metadata":{"id":"Q_qTGApCOs2r"}},{"cell_type":"code","source":["feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = MinMaxScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)"],"metadata":{"id":"FdkyIkaiOsk1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's important to clarify the terminology, as \"normalization\" can sometimes be a bit ambiguous. Here's a breakdown:\n","\n","* **Min-max scaling:**\n","    * This technique transforms data to fit within a specific range, typically between 0 and 1.\n","    * It's done by subtracting the minimum value of the feature and then dividing by the range (maximum value minus minimum value).\n","    * Therefore it is a form of Normalization.\n","    * This is what is most commonly refered to as normalization.\n","* **Normalization (in a broader sense):**\n","    * \"Normalization\" can refer to a wider range of scaling techniques, including min-max scaling.\n","    * However, sometimes \"normalization\" can also be used to refer to techniques that change the shape of a distributions, such as making it closer to a normal distribution.\n","* **Standardization (Z-score normalization):**\n","    * This technique transforms data to have a mean of 0 and a standard deviation of 1.\n","    * It's done by subtracting the mean and then dividing by the standard deviation.\n","    * This is distinctly different from min-max scaling.\n","\n","**In summary:**\n","\n","* Min-max scaling is a specific type of normalization, where the goal is to rescale the data to a fixed range.\n","* Therefore, in many cases, min max scaling is refered to as normalization.\n","* It is very important to understand that standard scaling is a different process than min max scaling.\n","\n","Therefore, while min-max scaling is a form of normalization, \"normalization\" itself can have broader meanings. It is very important to specify which form of normalization is being used, to prevent confusion.\n"],"metadata":{"id":"RGUVB-ZGPTKu"}},{"cell_type":"markdown","source":["## Outliers\n","\n","**Code Explanation**\n","\n","```python\n","feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = RobustScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)\n","```\n","\n","This code snippet applies robust scaling to a selected numerical column within a Pandas DataFrame (`df`) using `RobustScaler` from scikit-learn.\n","\n","1.  **`feat = str(df._get_numeric_data().idxmax(1)[0])`**:\n","    * This line, as previously discussed, selects the first numerical column in the dataframe, and stores its name as a string.\n","\n","2.  **`scaler = RobustScaler()`**:\n","    * Creates an instance of the `RobustScaler`. This scaler is designed to handle outliers by using statistics that are less sensitive to extreme values.\n","\n","3.  **`df[feat] = scaler.fit_transform(df[[feat]].values)`**:\n","    * `df[[feat]].values` selects the specified column (`feat`) as a DataFrame (to maintain dimensionality) and converts it to a NumPy array.\n","    * `scaler.fit_transform(...)` fits the scaler to the data (calculates the median and interquartile range) and then transforms the data (scales it using these robust statistics).\n","    * `df[feat] = ...` replaces the original column in the DataFrame with the robustly scaled values.\n","\n","**Why Addressing Outliers Is Important in Data Modeling**\n","\n","Outliers are data points that significantly deviate from the rest of the data. They can have a substantial impact on data modeling and analysis:\n","\n","* **Distorted Statistics:**\n","    * Outliers can skew statistical measures like the mean and standard deviation, leading to inaccurate representations of the data.\n","* **Reduced Model Accuracy:**\n","    * Many machine learning algorithms are sensitive to outliers. Outliers can cause models to overfit to extreme values, leading to poor generalization on unseen data.\n","* **Biased Model Coefficients:**\n","    * In linear models, outliers can disproportionately influence the regression coefficients, leading to biased predictions.\n","* **Increased Error:**\n","    * Outliers can increase the overall error of a model, making it less reliable.\n","* **Violated Assumptions:**\n","    * Some statistical models rely on assumptions about the distribution of the data (e.g., normality). Outliers can violate these assumptions, making the model invalid.\n","* **Misleading Visualizations:**\n","    * Outliers can distort visualizations, making it difficult to identify meaningful patterns in the data.\n","\n","**How to Address Outliers**\n","\n","* **Detection:**\n","    * Use visualization techniques (e.g., box plots, scatter plots) to identify outliers.\n","    * Use statistical methods (e.g., Z-scores, IQR) to detect outliers.\n","* **Removal:**\n","    * Remove outliers if they are clearly errors or anomalies.\n","    * Be cautious when removing outliers, as they may contain valuable information.\n","* **Transformation:**\n","    * Use data transformation techniques (e.g., log transformation, power transformation) to reduce the impact of outliers.\n","    * Use scaling methods like RobustScaler.\n","* **Imputation:**\n","    * Replace outliers with more representative values (e.g., median, trimmed mean).\n","* **Robust Algorithms:**\n","    * Use machine learning algorithms that are less sensitive to outliers (e.g., robust regression, decision trees).\n","\n","**When to Use RobustScaler**\n","\n","* As the code demonstrates, `RobustScaler` is particularly useful when you want to scale your data without being heavily influenced by outliers.\n","* It is a good choice when you know that your data contains outliers and you want to maintain the relative order and spread of the majority of your data points.\n","\n","In summary, addressing outliers is crucial for building robust and reliable data models. `RobustScaler` is a valuable tool for scaling data in the presence of outliers.\n"],"metadata":{"id":"GpyqDAi8PtnN"}},{"cell_type":"code","source":["feat = str(df._get_numeric_data().idxmax(1)[0])\n","scaler = RobustScaler()\n","df[feat] = scaler.fit_transform(df[[feat]].values)"],"metadata":{"id":"MfqCncGRPtaU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`RobustScaler` from scikit-learn is a very good option for handling outliers. Let's discuss why and compare it to other methods.\n","\n","**RobustScaler Explained**\n","\n","* **How it works:**\n","    * `RobustScaler` scales features using statistics that are robust to outliers.\n","    * It removes the median and scales the data according to the quantile range (by default, the interquartile range or IQR).\n","    * The IQR is the range between the 1st quartile (25th percentile) and the 3rd quartile (75th percentile).\n","    * By using the median and IQR, `RobustScaler` is less affected by extreme values than `StandardScaler`, which uses the mean and standard deviation.\n","\n","* **Why it's good for outliers:**\n","    * Outliers have a much smaller influence on the median and IQR than they do on the mean and standard deviation.\n","    * Therefore, `RobustScaler` provides more robust scaling when your data contains outliers.\n","\n","**When to Use RobustScaler:**\n","\n","* When your data contains outliers.\n","* When you want to reduce the impact of outliers on your scaling.\n","* When you want to maintain the relative order of your data points, even with outliers.\n","\n","**Comparison with Other Methods**\n","\n","1.  **StandardScaler:**\n","    * Very sensitive to outliers.\n","    * Not recommended when outliers are present.\n","\n","2.  **MinMaxScaler:**\n","    * Also sensitive to outliers.\n","    * Outliers can significantly affect the minimum and maximum values, leading to skewed scaling.\n","\n","3.  **QuantileTransformer:**\n","    * Can also handle outliers by transforming features to a uniform or normal distribution.\n","    * It does change the shape of the original distribution, which might not always be desired.\n","    * It can be very useful when you want to normalize the distribution of your features.\n","\n","4.  **PowerTransformer:**\n","    * Applies power transforms (Yeo-Johnson or Box-Cox) to make the data more Gaussian-like.\n","    * Can also help with outliers, especially when the data is skewed.\n","    * Changes the shape of the original distribution.\n","\n","**Recommendation**\n","\n","* For most cases where you have outliers and you want to scale the data without significantly altering the distribution, `RobustScaler` is an excellent choice.\n","* If you also want to normalize the distribution of your features, `QuantileTransformer` or `PowerTransformer` could be considered.\n","* Always visualize your data before and after scaling to understand the effects of the transformation.\n","\n","**In summary:**\n","\n","* `RobustScaler` is a strong contender for handling outliers during scaling.\n","* It is very important to consider the affect that each scaling method has on your data, and what your specific goals are.\n"],"metadata":{"id":"MZIw4ChIQcEi"}},{"cell_type":"markdown","source":["# Example"],"metadata":{"id":"5l5COKfQqhir"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Lw7SojmIAWo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n","\n","def identify_consts(df):\n","  constant_features = [\n","      feat for feat in df.columns if len(df[feat].unique()) == 1\n","  ]\n","  return constant_features\n","\n","def identify_quasi_consts(df, thresh=0.95):\n","  # quasi constant values\n","  quasi_consts = []\n","  for val in df.columns.sort_values():\n","      if (len(df[val].unique()) < 3 and max(df[val].value_counts(normalize=True)) > thresh):\n","          quasi_consts.append(val)\n","\n","  return quasi_consts\n","\n","def check_row_duplicates(df):\n","  # duplicate rows\n","  return len(df[df.duplicated(keep=False)])\n","\n","def check_col_duplicates(df):\n","  # check of duplicate columns\n","  duplicate_features = []\n","  for i in range(0, len(df.columns)):\n","      orig = df.columns[i]\n","\n","      for dupe in df.columns[i + 1:]:\n","          if df[orig].equals(df[dupe]):\n","              duplicate_features.append(dupe)\n","\n","  return duplicate_features\n","\n","def do_OHE(df):\n","  cat_features = []\n","  for feat in df.select_dtypes(include=['object', 'category']):\n","    if len(df[feat].value_counts()) < 3:\n","      df[feat] = df[feat].map({df[feat].value_counts().index[0]: 0, df[feat].value_counts().index[1]: 1})\n","      df[feat] = df[feat].astype(int)\n","    elif 2 < len(df[feat].value_counts()) < 6:\n","      cat_features.append(feat)\n","    elif len(df[feat].value_counts()) > 5:\n","      freq = df.groupby(feat, observed=False).size()/len(df)\n","      df[feat] = df[feat].map(freq)\n","\n","  ohe = OneHotEncoder(categories='auto', drop='first', sparse_output=False, handle_unknown='ignore')\n","  ohe_df = ohe.fit_transform(df[cat_features])\n","  ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out(cat_features))\n","  ohe_df.index = df.index\n","  df = df.join(ohe_df)\n","  df.drop(cat_features, axis=1, inplace=True)\n","  return df\n","  return df\n","\n","def handle_missing_values(df):\n","  df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n","  dfx = df.copy()\n","  for feat in df.columns[df.isnull().sum() > 1]:\n","    if feat in df_categorical_features:\n","      dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n","    else:\n","      if abs(df[feat].skew()) < .8:\n","        dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n","      else:\n","        dfx[feat] = df[feat].fillna(df[feat].median())\n","\n","  return dfx\n","\n","def handle_standard_scaler(df):\n","  feat = str(df._get_numeric_data().idxmax(1)[0])\n","  scaler = StandardScaler()\n","  df[feat] = scaler.fit_transform(df[[feat]].values)\n","  return df\n","\n","def handle_minmax_scaler(df):\n","  feat = str(df._get_numeric_data().idxmax(1)[0])\n","  scaler = MinMaxScaler()\n","  df[feat] = scaler.fit_transform(df[[feat]].values)\n","  return df\n","\n","def handle_outliers(df):\n","  feat = str(df._get_numeric_data().idxmax(1)[0])\n","  scaler = RobustScaler()\n","  df[feat] = scaler.fit_transform(df[[feat]].values)\n","  return df"]},{"cell_type":"markdown","source":["## Create the Data"],"metadata":{"id":"WBewK3LoolEN"}},{"cell_type":"code","source":["pip install Faker -q"],"metadata":{"id":"hWAgPcHYVrWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.datasets import make_regression\n","from faker import Faker\n","\n","fake = Faker()\n","\n","output = []\n","for x in range(100):\n","    sex = np.random.choice(['egg', 'seed'], p=[0.5, 0.5])\n","    output.append({\n","        'categorical_1': sex,\n","        'categorical_2': np.random.choice(['A', 'B', 'C']),\n","        'name_1': fake.first_name_female() if sex == 'egg' else fake.first_name_male(),\n","        'name_2': fake.last_name(),\n","        'zip_code': fake.zipcode(),\n","        'date': fake.date_of_birth(),\n","        'location': fake.state_abbr()\n","    })\n","\n","demographics = pd.DataFrame(output)\n","\n","def make_null(r, w):\n","    if random.randint(0, 99) < w:\n","        return np.nan\n","    else:\n","        return r\n","\n","# Generating features for linear regression with generic names\n","features, target = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n","\n","generic_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5']\n","random.shuffle(generic_cols)\n","df = pd.DataFrame(data=features, columns=generic_cols)\n","df['target_variable'] = target\n","\n","# Introduce non-linearities and interactions\n","df['feature_1_squared'] = df['feature_1']**2\n","df['interaction_1_2'] = df['feature_1'] * df['feature_2']\n","\n","# Apply transformations and add noise\n","df['target_variable'] = df['target_variable'] + np.random.normal(0, 5, 100)\n","df['feature_4'] = df['feature_4'].apply(lambda x: abs(x) if x < 0 else x)\n","\n","# Add missing values\n","for col in generic_cols:\n","    df[col] = df[col].apply(make_null, args=(10,))\n","\n","df = pd.concat([df, demographics], axis=1)\n","\n","print(df.shape)\n","print(df.info())\n","df.head()"],"metadata":{"id":"w7Uw8hjrVfaz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Some Prep Ideas\n","\n","* Missing Values\n","* Categorical Encodeing\n","* Duplicates\n","* Scaling"],"metadata":{"id":"-pOov8ycAk8Y"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import Lasso, LinearRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","X = df.drop(['target_variable'], axis=1)\n","y = df['target_variable']\n","\n","X.info()\n"],"metadata":{"id":"vS2nOB8WWKKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# missing values\n","df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n","dfx = X.copy()\n","for feat in df.columns[df.isnull().sum() > 1]:\n","  if feat in df_categorical_features:\n","    dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n","  else:\n","    if abs(df[feat].skew()) < .8:\n","      dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n","    else:\n","      dfx[feat] = df[feat].fillna(df[feat].median())\n","\n","X = dfx.copy()\n","X.info()"],"metadata":{"id":"e5JLUVaW17hr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PrePy\n","\n","https://github.com/gitmystuff/preppy/tree/main"],"metadata":{"id":"lhE79hmPK6gB"}},{"cell_type":"code","source":["! git clone https://github.com/gitmystuff/preppy.git"],"metadata":{"id":"UphxjdyeUNk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from preppy.version import __version__\n","print(__version__)"],"metadata":{"id":"VQ2lHgvlUNS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# categorical encoding\n","import preppy.utils as preppy\n","\n","X = preppy.functions.do_OHE(X)\n","\n","dupes = X.loc[:7]\n","X = pd.concat([X, dupes], axis=0)\n","\n","X.info()"],"metadata":{"id":"Ha1QHYyQ11-m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipelines"],"metadata":{"id":"dW3yAtZ3bJ4Y"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class DuplicateRemoverDF(BaseEstimator, TransformerMixin):\n","    \"\"\"\n","    A transformer that removes duplicate rows and columns from a Pandas DataFrame.\n","\n","    Parameters:\n","    -----------\n","    remove_rows : bool, optional\n","        Whether to remove duplicate rows. Defaults to True.\n","    remove_columns : bool, optional\n","        Whether to remove duplicate columns. Defaults to True.\n","\n","    Attributes:\n","    -----------\n","    original_columns_ : list\n","        The original column indices before removing duplicates.\n","    original_rows_ : list\n","        The original row indices before removing duplicates.\n","\n","    \"\"\"\n","\n","    def __init__(self, remove_rows=True, remove_columns=True):\n","        self.remove_rows = remove_rows\n","        self.remove_columns = remove_columns\n","        self.original_columns_ = None\n","        self.original_rows_ = None\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fits the transformer to the input data.\n","\n","        Parameters:\n","        -----------\n","        X : pandas DataFrame of shape (n_samples, n_features)\n","            The input data.\n","        y : array-like of shape (n_samples,), optional\n","            The target values. Ignored.\n","\n","        Returns:\n","        --------\n","        self : object\n","            Returns the instance itself.\n","        \"\"\"\n","        if self.remove_columns:\n","            _, self.original_columns_ = np.unique(X.values, axis=1, return_index=True)\n","        if self.remove_rows:\n","            _, self.original_rows_ = np.unique(X.values, axis=0, return_index=True)\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transforms the input data by removing duplicate rows and columns.\n","\n","        Parameters:\n","        -----------\n","        X : pandas DataFrame of shape (n_samples, n_features)\n","            The input data.\n","\n","        Returns:\n","        --------\n","        X_transformed : pandas DataFrame of shape (n_samples_transformed, n_features_transformed)\n","            The transformed data.\n","        \"\"\"\n","        if self.remove_columns:\n","            X = X.iloc[:, self.original_columns_]\n","        if self.remove_rows:\n","            X = X.iloc[self.original_rows_, :]\n","        return X\n","\n","    def fit_transform(self, X, y=None, **fit_params):\n","        \"\"\"\n","        Fits to data, then transforms it.\n","        Fits transformer to X and y with optional parameters fit_params\n","        and returns a transformed version of X.\n","\n","        Parameters\n","        ----------\n","        X : pandas DataFrame of shape (n_samples, n_features)\n","            Input samples.\n","        y : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n","            default=None\n","            Target values (ignored).\n","        **fit_params : dict\n","            Additional fit parameters.\n","\n","        Returns\n","        -------\n","        X_new : pandas DataFrame of shape (n_samples_new, n_features_new)\n","            Transformed array.\n","        \"\"\"\n","        self.fit(X, y)\n","        return self.transform(X)\n"],"metadata":{"id":"D0QiLRh_5aTJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1. `BaseEstimator`**\n","\n","* **Purpose:**\n","    * `BaseEstimator` is a base class in scikit-learn that provides a standard interface for all estimators (including transformers and models).\n","    * It primarily ensures that your custom classes adhere to scikit-learn's conventions and can be used seamlessly within its ecosystem.\n","* **Key Features:**\n","    * **`get_params()`:** This method allows scikit-learn to retrieve the parameters of your estimator. It's crucial for functions like `GridSearchCV` and `Pipeline`, which need to inspect and manipulate the parameters.\n","    * **`set_params()`:** This method allows scikit-learn to set the parameters of your estimator. It's used by `GridSearchCV` and `Pipeline` to configure the estimator during the search or pipeline execution.\n","    * By inheriting from `BaseEstimator`, you gain these core functionalities without having to implement them yourself.\n","* **Why It's Important:**\n","    * Consistency: It enforces a consistent API, making your custom estimators compatible with scikit-learn's tools.\n","    * Integration: It enables your estimators to work smoothly with pipelines, cross-validation, and hyperparameter tuning.\n","\n","**2. `TransformerMixin`**\n","\n","* **Purpose:**\n","    * `TransformerMixin` is a mixin class specifically designed for transformers.\n","    * It provides a default implementation of the `fit_transform()` method.\n","* **Key Features:**\n","    * **`fit_transform(X, y=None, **fit_params)`:** This method combines the `fit()` and `transform()` steps into a single call.\n","        * The default implementation simply calls `self.fit(X, y, **fit_params)` followed by `self.transform(X)`.\n","        * You can override `fit_transform()` if you need a more efficient or customized implementation.\n","* **Why It's Important:**\n","    * Convenience: It saves you from having to write the `fit_transform()` method explicitly in every transformer.\n","    * Efficiency: While the default implementation works, you can optimize it for your specific transformer if needed.\n","    * By inheriting from TransformerMixin, you are stating that your class will be able to transform data.\n","\n","**In summary:**\n","\n","* When you create a custom estimator (like a transformer or a model), you should inherit from `BaseEstimator` to ensure it integrates well with scikit-learn.\n","* If your estimator is a transformer (i.e., it transforms data), you should also inherit from `TransformerMixin` to get the default `fit_transform()` implementation.\n","\n","By using these mixin classes, you make your custom code more robust, maintainable, and compatible with the scikit-learn ecosystem.\n"],"metadata":{"id":"hlIAWS8BacHr"}},{"cell_type":"code","source":["duplicate_remover = DuplicateRemoverDF()\n","X = duplicate_remover.fit_transform(X)\n","\n","X.info()"],"metadata":{"id":"s0J5UOTw3i_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline = Pipeline([\n","    ('constants', preppy.classes.ConstantAndSemiConstantRemover()),\n","])\n","\n","X = pipeline.fit_transform(X)\n","X.info()"],"metadata":{"id":"h3dF5Et17Bu-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lasso Model"],"metadata":{"id":"SH8tzncLbW9q"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.20,\n","                                                    random_state=42)\n","\n","\n","pipe = [\n","    ('scalar', StandardScaler()),\n","    ('model', Lasso(alpha=10, fit_intercept=True))\n","]\n","\n","model = Pipeline(pipe)\n","model.fit(X_train, y_train)\n","\n","# model = Lasso(alpha=10, fit_intercept=True)\n","# # model = LinearRegression()\n","# model.fit(X_train, y_train)\n","\n","d = {'Feature': X_train.columns, 'Coef': model.named_steps['model'].coef_}\n","pipe_df = pd.DataFrame(d)\n","print(pipe_df)"],"metadata":{"id":"9bDPkHEXLbmX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics"],"metadata":{"id":"fQ6Yjml5baUE"}},{"cell_type":"code","source":["predictions = model.predict(X_test.dropna())\n","\n","print(f'Model Training Score (R^2): {model.score(X_train, y_train)}')\n","print(f'Model Test Score (R^2): {model.score(X_test, y_test)}')\n","print(f'Model Predictions Score: {r2_score(y_test, predictions)}')\n","\n","mae = mean_absolute_error(y_test, predictions)\n","mse = mean_squared_error(y_test, predictions)\n","rmse = np.sqrt(mse)\n","print()\n","print(f'MAE: {mae}')\n","print(f'MSE: {mse}')\n","print(f'RMSE: {rmse}')\n","\n","# Prediction DataFrame\n","prediction_df = pd.DataFrame({'Actual': y_test, 'Predicted': predictions, 'Residuals': y_test - predictions})\n","print('\\nPrediction DataFrame:')\n","print(prediction_df.head())\n","\n","# Visualizations\n","plt.figure(figsize=(12, 6))\n","\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(x='Actual', y='Predicted', data=prediction_df)\n","plt.title('Actual vs. Predicted')\n","\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(x='Predicted', y='Residuals', data=prediction_df)\n","plt.axhline(y=0, color='r', linestyle='--')\n","plt.title('Residual Plot')\n","\n","plt.show()"],"metadata":{"id":"ZZ3De8HpHo_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## OLS Model"],"metadata":{"id":"269sbtWBbdcZ"}},{"source":["import statsmodels.api as sm\n","\n","X_train = X_train.copy()\n","X_train.insert(0, 'const', 1)\n","X_with_const = X_train\n","\n","X_with_const = X_with_const.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","ols_model = sm.OLS(y_train, X_with_const).fit()\n","print(ols_model.summary())"],"cell_type":"code","metadata":{"id":"fDMgVID_PqXo"},"execution_count":null,"outputs":[]}]}